---
title: 内存优化表的持久性 | Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: in-memory-oltp
ms.topic: conceptual
ms.assetid: d304c94d-3ab4-47b0-905d-3c8c2aba9db6
author: CarlRabeler
ms.author: carlrab
ms.openlocfilehash: 1d48d671b23d7b7b17557e7829d6f2522c375acd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 08/04/2020
ms.locfileid: "87692414"
---
# <a name="durability-for-memory-optimized-tables"></a><span data-ttu-id="02fe3-102">内存优化表的持续性</span><span class="sxs-lookup"><span data-stu-id="02fe3-102">Durability for Memory-Optimized Tables</span></span>
  [!INCLUDE[hek_2](../../../includes/hek-2-md.md)] <span data-ttu-id="02fe3-103">为内存优化表提供完整持续性。</span><span class="sxs-lookup"><span data-stu-id="02fe3-103">provides full durability for memory-optimized tables.</span></span> <span data-ttu-id="02fe3-104">提交更改内存优化表的事务时，假设基础存储可用， [!INCLUDE[ssNoVersion](../../../includes/ssnoversion-md.md)] （就像对基于磁盘的表一样）会保证更改是永久的（数据库重新启动时仍然有效）。</span><span class="sxs-lookup"><span data-stu-id="02fe3-104">When a transaction that changed a memory-optimized table commits, [!INCLUDE[ssNoVersion](../../../includes/ssnoversion-md.md)] (as it does for disk-based tables), guarantees that the changes are permanent (will survive a database restart), provided the underlying storage is available.</span></span> <span data-ttu-id="02fe3-105">持续性有两个重要方面：事务日志记录和在磁盘存储上持久保存数据更改。</span><span class="sxs-lookup"><span data-stu-id="02fe3-105">There are two key components of durability: transaction logging and persisting data changes to on-disk storage.</span></span>

## <a name="transaction-log"></a><span data-ttu-id="02fe3-106">事务日志</span><span class="sxs-lookup"><span data-stu-id="02fe3-106">Transaction Log</span></span>
 <span data-ttu-id="02fe3-107">对基于磁盘的表或持久的内存优化表所做的所有更改均捕获在一个或多个事务日志记录中。</span><span class="sxs-lookup"><span data-stu-id="02fe3-107">All changes made to disk-based tables or durable memory-optimized tables are captured in one or more transaction log records.</span></span> <span data-ttu-id="02fe3-108">提交事务时， [!INCLUDE[ssNoVersion](../../../includes/ssnoversion-md.md)] 将与事务关联的日志记录写入磁盘，然后再通知应用程序或用户会话事务已提交。</span><span class="sxs-lookup"><span data-stu-id="02fe3-108">When a transaction commits, [!INCLUDE[ssNoVersion](../../../includes/ssnoversion-md.md)] writes the log records associated with the transaction to disk before communicating to the application or user session that the transaction has committed.</span></span> <span data-ttu-id="02fe3-109">这样可确保事务所做的更改是持久的。</span><span class="sxs-lookup"><span data-stu-id="02fe3-109">This guarantees that changes made by the transaction are durable.</span></span> <span data-ttu-id="02fe3-110">内存优化表的事务日志与基于磁盘的表使用的相同日志流完全集成。</span><span class="sxs-lookup"><span data-stu-id="02fe3-110">The transaction log for memory-optimized tables is fully integrated with the same log stream used by disk-based tables.</span></span> <span data-ttu-id="02fe3-111">此集成使现有事务日志备份、恢复和还原操作可以继续执行，无需任何附加步骤。</span><span class="sxs-lookup"><span data-stu-id="02fe3-111">This integration allows existing transaction log backup, recover, and restore operations to continue to work without requiring any additional steps.</span></span> <span data-ttu-id="02fe3-112">但是，由于 [!INCLUDE[hek_2](../../../includes/hek-2-md.md)] 可显著增加工作负荷的事务吞吐量，因此您需要确保正确配置事务日志存储以应对提高的 IO 要求。</span><span class="sxs-lookup"><span data-stu-id="02fe3-112">However, since [!INCLUDE[hek_2](../../../includes/hek-2-md.md)] can increase transaction throughput of your workload significantly, you need to make sure that transaction log storage is configured appropriately to handle the increased IO requirements.</span></span>

## <a name="data-and-delta-files"></a><span data-ttu-id="02fe3-113">数据和差异文件</span><span class="sxs-lookup"><span data-stu-id="02fe3-113">Data and Delta Files</span></span>
 <span data-ttu-id="02fe3-114">内存优化表中的数据在内存中存储为自由格式的数据行，它们通过一个或多个内存中索引链接起来。</span><span class="sxs-lookup"><span data-stu-id="02fe3-114">The data in memory-optimized tables is stored as free-form data rows that are linked through one or more in-memory indexes, in memory.</span></span> <span data-ttu-id="02fe3-115">这些数据行没有基于磁盘的表所使用的页面结构。</span><span class="sxs-lookup"><span data-stu-id="02fe3-115">There are no page structures for data rows, such as those used for disk-based tables.</span></span> <span data-ttu-id="02fe3-116">应用程序准备提交事务时，[!INCLUDE[hek_2](../../../includes/hek-2-md.md)] 将生成该事务的日志记录。</span><span class="sxs-lookup"><span data-stu-id="02fe3-116">When the application is ready to commit the transaction, the [!INCLUDE[hek_2](../../../includes/hek-2-md.md)] generates the log records for the transaction.</span></span> <span data-ttu-id="02fe3-117">内存优化表的持续性使用后台线程通过一组数据文件和差异文件来实现。</span><span class="sxs-lookup"><span data-stu-id="02fe3-117">The persistence of memory-optimized tables is done with a set of data and delta files using a background thread.</span></span> <span data-ttu-id="02fe3-118">数据文件和差异文件位于一个或多个容器中（使用 FILESTREAM 数据所用的机制）。</span><span class="sxs-lookup"><span data-stu-id="02fe3-118">The data and delta files are located in one or more containers (using the same mechanism used for FILESTREAM data).</span></span> <span data-ttu-id="02fe3-119">这些容器映射到一个新类型的文件组，称为内存优化文件组。</span><span class="sxs-lookup"><span data-stu-id="02fe3-119">These containers are mapped to a new type of filegroup, called a memory-optimized filegroup.</span></span>

 <span data-ttu-id="02fe3-120">按严格顺序向这些文件写入数据，这样可将旋转介质的磁盘延迟降至最低。</span><span class="sxs-lookup"><span data-stu-id="02fe3-120">Data is written to these files in a strictly sequential fashion, which minimizes disk latency for spinning media.</span></span> <span data-ttu-id="02fe3-121">可使用不同驱动器上的多个容器分散 I/O 活动。</span><span class="sxs-lookup"><span data-stu-id="02fe3-121">You can use multiple containers on different disks to distribute the I/O activity.</span></span> <span data-ttu-id="02fe3-122">从磁盘上的数据和差异文件中将数据读入内存时，不同磁盘上多个容器中的数据和差异文件将提高恢复性能。</span><span class="sxs-lookup"><span data-stu-id="02fe3-122">Data and delta files in multiple containers on different disks will increase recovery performance when data is read from the data and delta files on disk, into memory.</span></span>

 <span data-ttu-id="02fe3-123">应用程序不直接访问数据和差异文件。</span><span class="sxs-lookup"><span data-stu-id="02fe3-123">An application does not directly access data and delta files.</span></span> <span data-ttu-id="02fe3-124">所有数据读取和写入均使用内存中数据。</span><span class="sxs-lookup"><span data-stu-id="02fe3-124">All data reads and writes use in-memory data.</span></span>

### <a name="the-data-file"></a><span data-ttu-id="02fe3-125">数据文件</span><span class="sxs-lookup"><span data-stu-id="02fe3-125">The Data File</span></span>
 <span data-ttu-id="02fe3-126">数据文件将包含一个或多个内存优化表中作为 INSERT 或 UPDATE 操作的一部分由多个事务插入的行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-126">A data file contains rows from one or more memory-optimized tables that were inserted by multiple transactions as part of INSERT or UPDATE operations.</span></span> <span data-ttu-id="02fe3-127">例如，一行可以来自内存优化表 T1，而下一行可以来自内存优化表 T2。</span><span class="sxs-lookup"><span data-stu-id="02fe3-127">For example, one row can be from memory-optimized table T1 and the next row can be from memory-optimized table T2.</span></span> <span data-ttu-id="02fe3-128">这些行按事务日志中事务的顺序追加到数据文件，使数据访问顺序进行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-128">The rows are appended to the data file in the order of transactions in the transaction log, making data access sequential.</span></span> <span data-ttu-id="02fe3-129">这将使 I/O 吞吐量比随机 I/O 提高一个数量级。</span><span class="sxs-lookup"><span data-stu-id="02fe3-129">This enables an order of magnitude better I/O throughput compared to random I/O.</span></span> <span data-ttu-id="02fe3-130">在内存大于 16 GB 的计算机中，每个数据文件的大小约为 128 MB；在内存小于或等于 16 GB 的计算机中，其大小约为 16 MB。</span><span class="sxs-lookup"><span data-stu-id="02fe3-130">Each data file is sized approximately to 128MB for computers with memory greater than 16GB, and 16MB for computers with less than or equal to 16GB.</span></span> <span data-ttu-id="02fe3-131">一旦数据文件已满，新事务插入的行就存储于其他数据文件中。</span><span class="sxs-lookup"><span data-stu-id="02fe3-131">Once the data file is full, the rows inserted by new transactions are stored in another data file.</span></span> <span data-ttu-id="02fe3-132">一段时间后，来自持久内存优化表中的行将存储于一个或多个数据文件中，并且每个数据文件都包含来自不相联事务范围（而非连续事务范围）的行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-132">Over time, the rows from durable memory-optimized tables are stored in one of more data files and each data file containing rows from a disjoint but contiguous range of transactions.</span></span> <span data-ttu-id="02fe3-133">例如，事务提交时间戳范围为 (100, 200) 的数据文件包含由提交时间戳大于 100 并小于等于 200 的事务插入的所有行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-133">For example a data file with transaction commit timestamp in the range of (100, 200) has all the rows inserted by transactions that have commit timestamp greater than 100 and less than or equal to 200.</span></span> <span data-ttu-id="02fe3-134">该提交时间戳是在可供提交时分配给某一事务的单调递增的数字。</span><span class="sxs-lookup"><span data-stu-id="02fe3-134">The commit timestamp is a monotonically increasing number assigned to a transaction when it is ready to commit.</span></span> <span data-ttu-id="02fe3-135">每个事务都具有唯一的提交时间戳。</span><span class="sxs-lookup"><span data-stu-id="02fe3-135">Each transaction has a unique commit timestamp.</span></span>

 <span data-ttu-id="02fe3-136">在删除或更新某一行时，该行并不在数据文件中就地删除或更改，而是在另一种文件类型（即差异文件）中跟踪已删除行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-136">When a row is deleted or updated, the row is not removed or changed in-place in the data file but the deleted rows are tracked in another type of file: the delta file.</span></span> <span data-ttu-id="02fe3-137">更新操作以针对每一行的删除和插入操作的元组形式进行处理。</span><span class="sxs-lookup"><span data-stu-id="02fe3-137">Update operations are processed as a tuple of delete and insert operations for each row.</span></span> <span data-ttu-id="02fe3-138">这将消除数据文件上的随机 IO。</span><span class="sxs-lookup"><span data-stu-id="02fe3-138">This eliminates random IO on the data file.</span></span>

### <a name="the-delta-file"></a><span data-ttu-id="02fe3-139">差异文件</span><span class="sxs-lookup"><span data-stu-id="02fe3-139">The Delta File</span></span>
 <span data-ttu-id="02fe3-140">每个数据文件都与一个差异文件配对，后者的事务范围相同，并跟踪事务范围中的事务插入的已删除行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-140">Each data file is paired with a delta file that has the same transaction range and tracks the deleted rows inserted by transactions in the transaction range.</span></span> <span data-ttu-id="02fe3-141">此数据和差异文件称为检查点文件对 (CFP)，是分配和释放的单位以及合并操作的单位。</span><span class="sxs-lookup"><span data-stu-id="02fe3-141">This data and delta file is referred to as a Checkpoint File Pair (CFP) and it is the unit of allocation and deallocation as well as the unit for Merge operations.</span></span> <span data-ttu-id="02fe3-142">例如，与事务范围 (100, 200) 对应的差异文件将存储范围 (100, 200) 内的事务插入的删除行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-142">For example, a delta file corresponding to transaction range (100, 200) will store deleted rows that were inserted by transactions in the range (100, 200).</span></span> <span data-ttu-id="02fe3-143">与数据文件一样，差异文件是顺序访问的。</span><span class="sxs-lookup"><span data-stu-id="02fe3-143">Like data files, the delta file is accessed sequentially.</span></span>

 <span data-ttu-id="02fe3-144">在删除某一行时，该行并不是从数据文件中删除，而是对该行的引用将追加到与插入了此数据行的事务范围相关联的差异文件。</span><span class="sxs-lookup"><span data-stu-id="02fe3-144">When a row is deleted, the row is not removed from the data file but a reference to the row is appended to the delta file associated with the transaction range where this data row was inserted.</span></span> <span data-ttu-id="02fe3-145">由于要删除的行已存在于数据文件中，因此差异文件仅存储引用信息 `{inserting_tx_id, row_id, deleting_tx_id }` ，并且遵循原始删除或更新操作的事务日志顺序。</span><span class="sxs-lookup"><span data-stu-id="02fe3-145">Since the row to be deleted already exists in the data file, the delta file only stores the reference information `{inserting_tx_id, row_id, deleting_tx_id }` and it follows the transactional log order of the originating delete or update operations.</span></span>

## <a name="populating-data-and-delta-files"></a><span data-ttu-id="02fe3-146">填充数据和差异文件</span><span class="sxs-lookup"><span data-stu-id="02fe3-146">Populating Data and Delta Files</span></span>
 <span data-ttu-id="02fe3-147">由称为脱机检查点的后台线程填充数据和差异文件。</span><span class="sxs-lookup"><span data-stu-id="02fe3-147">Data and delta file are populated by a background thread called offline checkpoint.</span></span> <span data-ttu-id="02fe3-148">此线程读取已提交事务在内存优化表上生成的事务日志记录，并且将与已插入行和已删除行有关的信息追加到相应的数据和差异文件中。</span><span class="sxs-lookup"><span data-stu-id="02fe3-148">This thread reads the transaction log records generated by committed transactions on memory-optimized tables and appends information about the inserted and deleted rows into appropriate data and delta files.</span></span> <span data-ttu-id="02fe3-149">与基于磁盘的表（在此类表中，在检查点操作完成时使用随机 I/O 刷新数据/索引页）不同，内存优化表的持续性是连续的后台操作。</span><span class="sxs-lookup"><span data-stu-id="02fe3-149">Unlike disk-based tables where data/index pages are flushed with random I/O when checkpoint is done, the persistence of memory-optimized table is continuous background operation.</span></span> <span data-ttu-id="02fe3-150">由于事务可以删除或更新由任何先前的事务插入的任何行，因此将对多个差异文件进行访问。</span><span class="sxs-lookup"><span data-stu-id="02fe3-150">Multiple delta files are accessed because a transaction can delete or update any row that was inserted by any previous transaction.</span></span> <span data-ttu-id="02fe3-151">删除信息始终追加到差异文件的末尾。</span><span class="sxs-lookup"><span data-stu-id="02fe3-151">Deletion information is always appended at the end of the delta file.</span></span> <span data-ttu-id="02fe3-152">例如，提交时间戳为 600 的事务将插入一个新行并删除由提交时间戳为 150、250 和 450 的事务插入的行，如下图所示。</span><span class="sxs-lookup"><span data-stu-id="02fe3-152">For example, a transaction with a commit timestamp of 600 inserts one new row and deletes rows inserted by transactions with a commit timestamp of 150, 250 and 450 as shown in the picture below.</span></span> <span data-ttu-id="02fe3-153">所有 4 个文件 I/O 操作（三个用于删除的行，一个用于新插入的行）都是针对相应的增量和数据文件进行的仅追加操作。</span><span class="sxs-lookup"><span data-stu-id="02fe3-153">All 4 file I/O operations (three for deleted rows and 1 for the newly inserted rows), are append-only operations to the corresponding delta and data files.</span></span>

 <span data-ttu-id="02fe3-154">![内存优化表的读取日志记录。](../../database-engine/media/read-logs-hekaton.gif "内存优化表的读取日志记录。")</span><span class="sxs-lookup"><span data-stu-id="02fe3-154">![Read log records for memory-optimized tables.](../../database-engine/media/read-logs-hekaton.gif "Read log records for memory-optimized tables.")</span></span>

## <a name="accessing-data-and-delta-files"></a><span data-ttu-id="02fe3-155">访问数据和差异文件</span><span class="sxs-lookup"><span data-stu-id="02fe3-155">Accessing Data and Delta Files</span></span>
 <span data-ttu-id="02fe3-156">发生以下情况时，将会访问数据和差异文件对。</span><span class="sxs-lookup"><span data-stu-id="02fe3-156">Data and delta file pairs are accessed when the following occurs.</span></span>

 <span data-ttu-id="02fe3-157">脱机检查点线程此线程将对内存优化数据行的插入和删除追加到对应的数据和差异文件对。</span><span class="sxs-lookup"><span data-stu-id="02fe3-157">Offline checkpoint thread This thread appends inserts and deletes to memory-optimized data rows, to the corresponding data and delta file pairs.</span></span>

 <span data-ttu-id="02fe3-158">合并操作该操作合并一个或多个数据和差异文件对，并创建新的数据和差异文件对。</span><span class="sxs-lookup"><span data-stu-id="02fe3-158">Merge operation The operation merges one or more data and delta file pairs and creates a new data and delta file pair.</span></span>

 <span data-ttu-id="02fe3-159">当 [!INCLUDE[ssNoVersion](../../../includes/ssnoversion-md.md)] 重新启动或数据库重新联机时，在崩溃恢复期间，将使用数据和差异文件对来填充内存优化数据。</span><span class="sxs-lookup"><span data-stu-id="02fe3-159">During crash recovery When [!INCLUDE[ssNoVersion](../../../includes/ssnoversion-md.md)] is restarted or the database is brought back online, the memory-optimized data is populated using the data and delta file pairs.</span></span> <span data-ttu-id="02fe3-160">从对应的数据文件读取行时，差异文件充当用于已删除行的筛选器。</span><span class="sxs-lookup"><span data-stu-id="02fe3-160">The delta file acts as a filter for the deleted rows when reading the rows from the corresponding data file.</span></span> <span data-ttu-id="02fe3-161">由于每个数据和差异文件对均独立，因此并行加载这些文件以缩短将数据填充到内存中所用的时间。</span><span class="sxs-lookup"><span data-stu-id="02fe3-161">Because each data and delta file pair is independent, these files are loaded in parallel to reduce the time taken to populate data into memory.</span></span> <span data-ttu-id="02fe3-162">将数据载入内存后，内存中 OLTP 引擎即应用检查点文件尚未涵盖的活动事务日志记录，以使内存优化的数据完整。</span><span class="sxs-lookup"><span data-stu-id="02fe3-162">Once the data has been loaded into memory, the In-Memory OLTP engine applies the active transaction log records not yet covered by the checkpoint files so that the memory-optimized data is complete.</span></span>

 <span data-ttu-id="02fe3-163">在还原操作过程中，将从数据库备份创建内存中 OLTP 检查点文件，然后应用一个或多个事务日志备份。</span><span class="sxs-lookup"><span data-stu-id="02fe3-163">During restore operation The In-Memory OLTP checkpoint files are created from the database backup, and then one or more transaction log backups are applied.</span></span> <span data-ttu-id="02fe3-164">与崩溃恢复一样，内存中 OLTP 引擎将数据并行加载到内存中，以最大程度减小对恢复时间的影响。</span><span class="sxs-lookup"><span data-stu-id="02fe3-164">As with crash recovery, the In-Memory OLTP engine loads data into memory in parallel, to minimize the impact on recovery time.</span></span>

## <a name="merging-data-and-delta-files"></a><span data-ttu-id="02fe3-165">迁移数据和差异文件</span><span class="sxs-lookup"><span data-stu-id="02fe3-165">Merging Data and Delta Files</span></span>
 <span data-ttu-id="02fe3-166">内存优化表的数据存储于一个或多个数据和差异文件对（也称作检查点文件对，即 CFP）中。</span><span class="sxs-lookup"><span data-stu-id="02fe3-166">The data for memory optimized tables is stored in one or more data and delta file pairs (also called a checkpoint file pair, or CFP).</span></span> <span data-ttu-id="02fe3-167">数据文件存储插入的行，而差异文件引用删除的行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-167">Data files store inserted rows and delta files reference deleted rows.</span></span> <span data-ttu-id="02fe3-168">在 OLTP 工作负荷执行期间，在 DML 操作更新、插入和删除行时，将创建新 CFP 以便持久保存新行，并将对已删除行的引用附加到差异文件中。</span><span class="sxs-lookup"><span data-stu-id="02fe3-168">During the execution of an OLTP workload, as the DML operations update, insert, and delete rows, new CFPs are created to persist the new rows, and the reference to the deleted rows is appended to delta files.</span></span>

 <span data-ttu-id="02fe3-169">所有以前关闭的和当前活动的 CFP 的元数据都存储于称作存储数组的内部数组结构中。</span><span class="sxs-lookup"><span data-stu-id="02fe3-169">The metadata of all previously-closed and currently active CFPs is stored in an internal array structure referred to as the storage array.</span></span> <span data-ttu-id="02fe3-170">它是有限大小（8,192 项）的 CFP 数组。</span><span class="sxs-lookup"><span data-stu-id="02fe3-170">It is a finitely sized (8,192 entries) array of CFPs.</span></span> <span data-ttu-id="02fe3-171">该存储数组中的项按事务范围排序。</span><span class="sxs-lookup"><span data-stu-id="02fe3-171">The entries in the storage array are ordered by transaction range.</span></span> <span data-ttu-id="02fe3-172">该存储数组中的 CFP（以及日志尾部）表示恢复含内存优化表的数据库所需的所有磁盘上的状态。</span><span class="sxs-lookup"><span data-stu-id="02fe3-172">The CFPs in the storage array (along with the tail of the log) represent all the on-disk state required to recover a database with memory-optimized tables.</span></span>

 <span data-ttu-id="02fe3-173">随着 DML 操作的不断执行，CFP 数量逐渐增多，将导致存储数组到达容量上限，从而产生以下挑战：</span><span class="sxs-lookup"><span data-stu-id="02fe3-173">Over time, with DML operations, the number of CFPs grow causing the storage array to reach capacity, which introduces the following challenges:</span></span>

-   <span data-ttu-id="02fe3-174">删除了行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-174">Deleted rows.</span></span>  <span data-ttu-id="02fe3-175">删除的行保留在数据文件中，但在相应的差异文件中标记为删除。</span><span class="sxs-lookup"><span data-stu-id="02fe3-175">Deleted rows remain in the data file but are marked as deleted in the corresponding delta file.</span></span> <span data-ttu-id="02fe3-176">这些行将不再需要并且将从存储中删除。</span><span class="sxs-lookup"><span data-stu-id="02fe3-176">These rows are no longer needed and will be removed from the storage.</span></span> <span data-ttu-id="02fe3-177">如果删除的行未从 CFP 中删除，则它们将不必要地占用空间并且延长恢复时间。</span><span class="sxs-lookup"><span data-stu-id="02fe3-177">If deleted rows were not removed from CFPs, they would use space unnecessarily and make recovery time slower.</span></span>

-   <span data-ttu-id="02fe3-178">存储数组已满。</span><span class="sxs-lookup"><span data-stu-id="02fe3-178">Storage array full.</span></span> <span data-ttu-id="02fe3-179">在存储数组中分配了 8,000 项后（数组中的 192 项为现有合并预留以进行争用或用来执行手动合并），将不能对持久内存优化表执行新的 DML 事务。</span><span class="sxs-lookup"><span data-stu-id="02fe3-179">When there 8,000 entries in the storage array are allocated (192 entries in the array are reserved for existing merges to compete or to allow you to do manual merges), no new DML transactions can be executed on durable memory-optimized tables.</span></span> <span data-ttu-id="02fe3-180">只允许检查点和合并操作使用剩余的项。</span><span class="sxs-lookup"><span data-stu-id="02fe3-180">Only checkpoint and merge operations are allowed to consume the remaining entries.</span></span> <span data-ttu-id="02fe3-181">这确保 DML 事务不会填满数组，并且确保保留数组中的某些项以便用于合并现有文件和回收数组中的空间。</span><span class="sxs-lookup"><span data-stu-id="02fe3-181">This ensures that DML transactions do not fill the array and that some entries in the array are reserved to merge existing files and to reclaim space in the array.</span></span>

-   <span data-ttu-id="02fe3-182">存储数组操作开销。</span><span class="sxs-lookup"><span data-stu-id="02fe3-182">Storage array manipulation overhead.</span></span> <span data-ttu-id="02fe3-183">内部进程搜索存储数组以便查找某些操作，例如查找差异文件以便追加与某一已删除行有关的信息。</span><span class="sxs-lookup"><span data-stu-id="02fe3-183">Internal processes search the storage array for operations such as finding the delta file to append information about a deleted row.</span></span> <span data-ttu-id="02fe3-184">这些操作的开销将随着项数的增加而增加。</span><span class="sxs-lookup"><span data-stu-id="02fe3-184">The cost of these operations increases with the number of entries.</span></span>

 <span data-ttu-id="02fe3-185">为了帮助避免发生这些效率低下的情形，将基于下面所述的合并策略合并较旧的已关闭 CFP，因此，将压缩存储数组以便表示具有数量减少的 CFP 的相同数据集。</span><span class="sxs-lookup"><span data-stu-id="02fe3-185">To help prevent these inefficiencies, the older closed CFPs are merged, based on a merge policy described below, so the storage array is compacted to represent the same set of data, with a reduced number of CFPs.</span></span>

 <span data-ttu-id="02fe3-186">数据库中所有持久表在内存中的总大小不应超过 250 GB。</span><span class="sxs-lookup"><span data-stu-id="02fe3-186">The total in-memory size of all durable tables in a database should not exceed 250 GB.</span></span> <span data-ttu-id="02fe3-187">使用高达 250 GB 内存的持久表，假设进行插入、删除和更新操作，将平均需要 500 GB 的存储空间。</span><span class="sxs-lookup"><span data-stu-id="02fe3-187">Durable tables that use up to 250 GB of memory will, assuming insert, delete, and update operations, require on average 500 GB of storage space.</span></span> <span data-ttu-id="02fe3-188">要支持 500 GB 的存储空间，内存优化文件组中需要有 4,000 个数据和差异文件对。</span><span class="sxs-lookup"><span data-stu-id="02fe3-188">4,000 data and delta file pairs in the memory-optimized file group are required to support the 500 GB of storage space.</span></span>

 <span data-ttu-id="02fe3-189">数据库活动的短期激增可能导致检查点和合并操作滞后，这将增加需要的数据和差异文件对的数目。</span><span class="sxs-lookup"><span data-stu-id="02fe3-189">Short-term surges in database activity may cause checkpoint and merge operations lag, which will increase the number of required data and delta file pairs.</span></span> <span data-ttu-id="02fe3-190">为适应数据库活动的短期激增高峰，存储系统最多可分配 8,000 个数据和差异文件对，存储空间总计高达 1TB。</span><span class="sxs-lookup"><span data-stu-id="02fe3-190">To accommodate short-term surges spikes in database activity, the storage system can allocate up to 8,000 data and delta file pairs up to a total of 1TB of storage.</span></span> <span data-ttu-id="02fe3-191">到达此限值时，只能等待检查点操作赶上来，才允许在数据库上执行新事务。</span><span class="sxs-lookup"><span data-stu-id="02fe3-191">When that limit is reached, there will be no new transactions allowed on the database until checkpoint operations catch up.</span></span> <span data-ttu-id="02fe3-192">如果内存中持久表的大小长时间超过 250 GB，有可能达到 8,000 个文件对的上限。</span><span class="sxs-lookup"><span data-stu-id="02fe3-192">If the size of durable tables in memory exceeds 250GB for long periods of time, there is a chance of reaching the 8,000 file pair limit.</span></span>

 <span data-ttu-id="02fe3-193">合并操作将基于内部定义的合并策略将一个或更多的相邻的已关闭 CFP（称作合并源）作为输入，生成一个最终的 CFP，称作合并目标。</span><span class="sxs-lookup"><span data-stu-id="02fe3-193">The merge operation takes as input one or more adjacent closed CFPs (called merge source) based on an internally defined merge policy, and produces one resultant CFP, called the merge target.</span></span> <span data-ttu-id="02fe3-194">将使用源 CFP 的每个差异文件中的项从相应数据文件中筛选行，以便删除不需要的数据行。</span><span class="sxs-lookup"><span data-stu-id="02fe3-194">The entries in each delta file of the source CFPs are used to filter rows from the corresponding data file to remove the data rows that are not needed.</span></span> <span data-ttu-id="02fe3-195">源 CFP 中的其余行将合并到一个目标 CFP 中。</span><span class="sxs-lookup"><span data-stu-id="02fe3-195">The remaining rows in the source CFPs are consolidated into one target CFP.</span></span> <span data-ttu-id="02fe3-196">在该合并完成后，最终生成的合并目标 CFP 将替代源 CFP（合并源）。</span><span class="sxs-lookup"><span data-stu-id="02fe3-196">After the merge is complete, the resultant merge-target CFP replaces the source CFPs (merge sources).</span></span> <span data-ttu-id="02fe3-197">这些合并源 CFP 将首先经历过渡阶段，然后从存储中删除。</span><span class="sxs-lookup"><span data-stu-id="02fe3-197">The merge-source CFPs go through a transition phase before they are removed from storage.</span></span>

 <span data-ttu-id="02fe3-198">在下面的示例中，内存优化表文件组在时间戳 500 处具有四个数据和差异文件对，并且包含来自之前事务的数据。</span><span class="sxs-lookup"><span data-stu-id="02fe3-198">In the example below, the memory-optimized table file group has four data and delta file pairs at timestamp 500 containing data from previous transactions.</span></span> <span data-ttu-id="02fe3-199">例如，第一个数据文件中的行对应于时间戳大于 100 且小于等于 200 的事务，或者表示为 (100，200]。</span><span class="sxs-lookup"><span data-stu-id="02fe3-199">For example, the rows in the first data file correspond to transactions with timestamp greater than 100 and less than or equal to 200; alternatively represented as (100, 200].</span></span> <span data-ttu-id="02fe3-200">考虑到标为已删除的行后，将第二个和第三个数据文件显示为完整程度小于 50%。</span><span class="sxs-lookup"><span data-stu-id="02fe3-200">The second and third data files are shown to be less than 50 percent full after accounting for the rows marked as deleted.</span></span> <span data-ttu-id="02fe3-201">合并操作合并这两个 CFP 并且创建一个新 CFP，它包含时间戳大于 200 且小于等于 400 的事务，这是这两个 CFP 的合并后的范围。</span><span class="sxs-lookup"><span data-stu-id="02fe3-201">The merge operation combines these two CFPs and creates a new CFP containing transactions with timestamp greater than 200 and less than or equal to 400, which is the combined range of these two CFPs.</span></span> <span data-ttu-id="02fe3-202">您将看到另一个具有范围 (500, 600] 的 CFP，并且用于事务范围 (200, 400] 的非空差异文件显示，可与事务性活动（包括从源 CFP 删除更多行）同时完成合并操作。</span><span class="sxs-lookup"><span data-stu-id="02fe3-202">You see another CFP with range (500, 600] and non-empty delta file for transaction range (200, 400] shows that merge operation can be done concurrently with transactional activity including deleting more rows from the source CFPs.</span></span>

 <span data-ttu-id="02fe3-203">![图中显示了内存优化表文件组](../../database-engine/media/storagediagram-hekaton.png "图中显示了内存优化表文件组")</span><span class="sxs-lookup"><span data-stu-id="02fe3-203">![Diagram shows memory optimized table file group](../../database-engine/media/storagediagram-hekaton.png "Diagram shows memory optimized table file group")</span></span>

 <span data-ttu-id="02fe3-204">后台线程使用合并策略计算所有关闭的 CFP，然后启动一个或多个合并请求以便验证 CFP 的资格。</span><span class="sxs-lookup"><span data-stu-id="02fe3-204">A background thread evaluates all closed CFPs using a merge policy and then initiates one or more merge requests for the qualifying CFPs.</span></span> <span data-ttu-id="02fe3-205">这些合并请求由脱机检查点线程处理。</span><span class="sxs-lookup"><span data-stu-id="02fe3-205">These merge requests are processed by the offline checkpoint thread.</span></span> <span data-ttu-id="02fe3-206">将定期进行对合并策略的评估，并且在关闭检查点时也会进行评估。</span><span class="sxs-lookup"><span data-stu-id="02fe3-206">The evaluation of merge policy is done periodically and also when a checkpoint is closed.</span></span>

### <a name="sssql14-merge-policy"></a>[!INCLUDE[ssSQL14](../../../includes/sssql14-md.md)] <span data-ttu-id="02fe3-207">合并策略</span><span class="sxs-lookup"><span data-stu-id="02fe3-207">Merge Policy</span></span>
 [!INCLUDE[ssSQL14](../../../includes/sssql14-md.md)] <span data-ttu-id="02fe3-208">实现以下合并策略：</span><span class="sxs-lookup"><span data-stu-id="02fe3-208">implements the following merge policy:</span></span>

-   <span data-ttu-id="02fe3-209">在考虑已删除的行后，如果可以合并 2 个或更多的连续 CFP，则计划进行合并，这样，最后生成的行可以适合于 1 个 CFP 的理想大小。</span><span class="sxs-lookup"><span data-stu-id="02fe3-209">A merge is scheduled if 2 or more consecutive CFPs can be consolidated, after accounting for deleted rows, such that the resultant rows can fit into 1 CFP of ideal size.</span></span> <span data-ttu-id="02fe3-210">CFP 的理想大小按如下方式确定：</span><span class="sxs-lookup"><span data-stu-id="02fe3-210">The ideal size of CFP is determined as follows:</span></span>

    -   <span data-ttu-id="02fe3-211">如果计算机的内存小于或等于 16GB，则数据文件是 16MB，差异文件是 1MB。</span><span class="sxs-lookup"><span data-stu-id="02fe3-211">If a computer has less than or equal to 16GB of memory, the data file is 16MB and delta file is 1MB.</span></span>

    -   <span data-ttu-id="02fe3-212">如果计算机的内存大于 16GB，则数据文件是 128MB，差异文件是 16MB。</span><span class="sxs-lookup"><span data-stu-id="02fe3-212">If a computer has greater than 16GB of memory, the data file is 128MB and delta file is 16MB.</span></span>

-   <span data-ttu-id="02fe3-213">如果数据文件超过 256 MB，并且删除的行超过一半，则单个 CFP 可自行合并。</span><span class="sxs-lookup"><span data-stu-id="02fe3-213">A single CFP can be self-merged if the data file exceeds 256 MB and more than half of the rows are deleted.</span></span> <span data-ttu-id="02fe3-214">数据文件可能增大到大于 128MB（例如，如果单个事务或多个并发事务插入或更新大量数据，迫使数据文件增大为超过其理想大小），因为事务不能跨多个 CFP。</span><span class="sxs-lookup"><span data-stu-id="02fe3-214">A data file can grow larger than 128MB if, for example, a single transaction or multiple concurrent transactions inserts or updates large amount of data, forcing the data file to grow beyond its ideal size because a transaction cannot span multiple CFPs.</span></span>

 <span data-ttu-id="02fe3-215">下面是一些示例，显示将在合并策略下合并的 CFP：</span><span class="sxs-lookup"><span data-stu-id="02fe3-215">Here are some examples that show the CFPs that will be merged under the merge policy:</span></span>

|<span data-ttu-id="02fe3-216">相邻的 CFP 源文件（完整程度百分比）</span><span class="sxs-lookup"><span data-stu-id="02fe3-216">Adjacent CFPs Source Files (% full)</span></span>|<span data-ttu-id="02fe3-217">合并选择</span><span class="sxs-lookup"><span data-stu-id="02fe3-217">Merge Selection</span></span>|
|-------------------------------------------|---------------------|
|<span data-ttu-id="02fe3-218">CFP0 (30%), CFP1 (50%), CFP2 (50%), CFP3 (90%)</span><span class="sxs-lookup"><span data-stu-id="02fe3-218">CFP0 (30%), CFP1 (50%), CFP2 (50%), CFP3 (90%)</span></span>|<span data-ttu-id="02fe3-219">(CFP0, CFP1)</span><span class="sxs-lookup"><span data-stu-id="02fe3-219">(CFP0, CFP1)</span></span><br /><br /> <span data-ttu-id="02fe3-220">未选择 CFP2，因为它将使最后生成的数据文件超过 100% 的理想大小。</span><span class="sxs-lookup"><span data-stu-id="02fe3-220">CFP2 is not chosen as it will make resultant data file greater than 100% of the ideal size.</span></span>|
|<span data-ttu-id="02fe3-221">CFP0 (30%), CFP1 (20%), CFP2 (50%), CFP3 (10%)</span><span class="sxs-lookup"><span data-stu-id="02fe3-221">CFP0 (30%), CFP1 (20%), CFP2 (50%), CFP3 (10%)</span></span>|<span data-ttu-id="02fe3-222">(CFP0, CFP1, CFP2)。</span><span class="sxs-lookup"><span data-stu-id="02fe3-222">(CFP0, CFP1, CFP2).</span></span> <span data-ttu-id="02fe3-223">从左侧开始选择文件。</span><span class="sxs-lookup"><span data-stu-id="02fe3-223">Files are chosen starting from left.</span></span><br /><br /> <span data-ttu-id="02fe3-224">未选择 CTP3，因为它将使最后生成的数据文件超过 100% 的理想大小。</span><span class="sxs-lookup"><span data-stu-id="02fe3-224">CTP3 is not chosen as it will make resultant data file greater than 100% of the ideal size.</span></span>|
|<span data-ttu-id="02fe3-225">CFP0 (80%), CFP1 (30%), CFP2 (10%), CFP3 (40%)</span><span class="sxs-lookup"><span data-stu-id="02fe3-225">CFP0 (80%), CFP1 (30%), CFP2 (10%), CFP3 (40%)</span></span>|<span data-ttu-id="02fe3-226">(CFP1, CFP2, CFP3)。</span><span class="sxs-lookup"><span data-stu-id="02fe3-226">(CFP1, CFP2, CFP3).</span></span> <span data-ttu-id="02fe3-227">从左侧开始选择文件。</span><span class="sxs-lookup"><span data-stu-id="02fe3-227">Files are chosen starting from left.</span></span><br /><br /> <span data-ttu-id="02fe3-228">将跳过 CFP0，因为如果与 CFP1 合并，最后生成的数据文件超过 100% 的理想大小。</span><span class="sxs-lookup"><span data-stu-id="02fe3-228">CFP0 is skipped because if combined with CFP1, the resultant data file will be greater than 100% of the ideal size.</span></span>|

 <span data-ttu-id="02fe3-229">不是具有可用空间的所有 CFP 都符合合并条件。</span><span class="sxs-lookup"><span data-stu-id="02fe3-229">Not all CFPs with available space qualify for merge.</span></span> <span data-ttu-id="02fe3-230">例如，如果两个相邻的 CFP 为 60% 满，则它们将不符合合并条件，并且其中每个 CFP都将有 40% 的未使用存储空间。</span><span class="sxs-lookup"><span data-stu-id="02fe3-230">For example, if two adjacent CFPs are 60% full, they will not qualify for merge and each of these CFPs will have 40% storage unused.</span></span> <span data-ttu-id="02fe3-231">在最差情形下，所有 CFP 都将为 50% 满，存储利用率仅为 50%。</span><span class="sxs-lookup"><span data-stu-id="02fe3-231">In the worst case, all CFPs will be 50% full, a storage utilization of only 50%.</span></span> <span data-ttu-id="02fe3-232">已删除的行可能由于 CFP 不符合合并条件而仍存在于存储中，但这些行可能已因内存中垃圾回收而从内存中删除。</span><span class="sxs-lookup"><span data-stu-id="02fe3-232">While the deleted rows may exist in storage because the CFPs don't qualify for merge, the deleted rows may have already been removed from memory by in-memory garbage collection.</span></span> <span data-ttu-id="02fe3-233">存储和内存的管理与垃圾回收是独立的。</span><span class="sxs-lookup"><span data-stu-id="02fe3-233">The management of storage and the memory is independent from garbage collection.</span></span> <span data-ttu-id="02fe3-234">活动 CFP（不是所有 CFP 都在更新）使用的内存可以最高达到内存中持久表大小的 2 倍。</span><span class="sxs-lookup"><span data-stu-id="02fe3-234">Storage taken by active CFPs (not all CFPs are being updated) can be up to 2 times larger than the size of durable tables in memory.</span></span>

 <span data-ttu-id="02fe3-235">如果需要，可以通过调用[sp_xtp_merge_checkpoint_files &#40;transact-sql&#41;](/sql/relational-databases/system-stored-procedures/sys-sp-xtp-merge-checkpoint-files-transact-sql)显式执行手动合并。</span><span class="sxs-lookup"><span data-stu-id="02fe3-235">If needed, a manual merge can be explicitly performed by calling [sys.sp_xtp_merge_checkpoint_files &#40;Transact-SQL&#41;](/sql/relational-databases/system-stored-procedures/sys-sp-xtp-merge-checkpoint-files-transact-sql).</span></span>

### <a name="life-cycle-of-a-cfp"></a><span data-ttu-id="02fe3-236">CFP 的生命周期</span><span class="sxs-lookup"><span data-stu-id="02fe3-236">Life Cycle of a CFP</span></span>
 <span data-ttu-id="02fe3-237">CPF 首先要经过若干状态，然后才能被释放。</span><span class="sxs-lookup"><span data-stu-id="02fe3-237">CPFs transition through several states before they can be deallocated.</span></span> <span data-ttu-id="02fe3-238">在任何给定时间，CFP 都处于以下阶段之一：PRECREATED、UNDER CONSTRUCTION、ACTIVE、MERGE TARGET、MERGED SOURCE、REQUIRED FOR BACKUP/HA、IN TRANSITION TO TOMBSTONE 和 TOMBSTONE。</span><span class="sxs-lookup"><span data-stu-id="02fe3-238">At any given time, the CFPs are in one of the following phases: PRECREATED, UNDER CONSTRUCTION, ACTIVE, MERGE TARGET, MERGED SOURCE, REQUIRED FOR BACKUP/HA, IN TRANSITION TO TOMBSTONE, and TOMBSTONE.</span></span> <span data-ttu-id="02fe3-239">有关这些阶段的说明，请参阅 [sys.dm_db_xtp_checkpoint_files (Transact SQL)](/sql/relational-databases/system-dynamic-management-views/sys-dm-db-xtp-checkpoint-files-transact-sql)。</span><span class="sxs-lookup"><span data-stu-id="02fe3-239">For a description of these phases, see [sys.dm_db_xtp_checkpoint_files &#40;Transact-SQL&#41;](/sql/relational-databases/system-dynamic-management-views/sys-dm-db-xtp-checkpoint-files-transact-sql).</span></span>

 <span data-ttu-id="02fe3-240">在考虑处于各种状态的 CFP 所占用的存储空间后，持久的内存优化表所占用的整个存储空间最高可为内存中表大小的 2 倍。</span><span class="sxs-lookup"><span data-stu-id="02fe3-240">After accounting for the storage taken by CFPs in various states, the overall storage taken by durable memory-optimized tables can be much larger than 2 times the size of the tables in memory.</span></span> <span data-ttu-id="02fe3-241">可以查询 DMV [sys. dm_db_xtp_checkpoint_files &#40;transact-sql&#41;](/sql/relational-databases/system-dynamic-management-views/sys-dm-db-xtp-checkpoint-files-transact-sql) ，以列出内存优化文件组中的所有 cfp，包括其阶段。</span><span class="sxs-lookup"><span data-stu-id="02fe3-241">The DMV [sys.dm_db_xtp_checkpoint_files &#40;Transact-SQL&#41;](/sql/relational-databases/system-dynamic-management-views/sys-dm-db-xtp-checkpoint-files-transact-sql) can be queried to list all the CFPs in the memory-optimized filegroup, including their phase.</span></span> <span data-ttu-id="02fe3-242">将 CFP 从 MERGE SOURCE 状态转换为 TOMBSTONE 及最终垃圾收集可能占用五个检查点，其中每个检查点后跟一个事务日志备份（如果数据库针对完整或大容量日志恢复模式进行了配置）。</span><span class="sxs-lookup"><span data-stu-id="02fe3-242">Transitioning CFPs from MERGE SOURCE state to TOMBSTONE and ultimately garbage collection can take up five checkpoints, with each checkpoint followed by a transaction log backup, if the database is configured for full or bulk-logged recovery model.</span></span>

 <span data-ttu-id="02fe3-243">您可以手动强制在检查点后进行日志备份，以便加快垃圾回收速度，但这将添加 5 个空 CFP（5 个数据/差异文件对，每个数据文件的大小为 128MB）。</span><span class="sxs-lookup"><span data-stu-id="02fe3-243">You can manually force the checkpoint followed by log backup to expedite the garbage collection but then this will add 5 empty CFPs (5 data/delta file pairs with data file of size 128MB each).</span></span> <span data-ttu-id="02fe3-244">在生产方案中，作为备份策略的一部分进行的自动检查点和日志备份可使 CFP 无缝通过这些阶段，而无需任何手动干预。</span><span class="sxs-lookup"><span data-stu-id="02fe3-244">In production scenarios, the automatic checkpoints and log backups taken as part of backup strategy will seamlessly transition CFPs through these phases without requiring any manual intervention.</span></span> <span data-ttu-id="02fe3-245">垃圾回收进程的影响是，具有内存优化表的数据库与其在内存中的大小相比，可能具有更大的存储大小。</span><span class="sxs-lookup"><span data-stu-id="02fe3-245">The impact of the garbage collection process is that databases with memory-optimized tables may have a larger storage size compared to its size in memory.</span></span> <span data-ttu-id="02fe3-246">CFP 是内存中的持久内存优化表大小的最多四倍并不少见。</span><span class="sxs-lookup"><span data-stu-id="02fe3-246">It is not uncommon for CFPs to be up to four times the size of the durable memory-optimized tables in memory.</span></span>

## <a name="see-also"></a><span data-ttu-id="02fe3-247">另请参阅</span><span class="sxs-lookup"><span data-stu-id="02fe3-247">See Also</span></span>
 [<span data-ttu-id="02fe3-248">创建和管理用于内存优化对象的存储</span><span class="sxs-lookup"><span data-stu-id="02fe3-248">Creating and Managing Storage for Memory-Optimized Objects</span></span>](creating-and-managing-storage-for-memory-optimized-objects.md)


