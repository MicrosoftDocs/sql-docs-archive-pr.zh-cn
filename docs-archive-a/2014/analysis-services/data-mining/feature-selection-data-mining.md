---
title: 功能选择 (数据挖掘) |Microsoft Docs
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 08/04/2020
ms.locfileid: "87588152"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="cd270-102">功能选择（数据挖掘）</span><span class="sxs-lookup"><span data-stu-id="cd270-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="cd270-103">*功能选择*是数据挖掘中常用的一种术语，用于描述可用于将输入减少到可管理的大小以进行处理和分析的工具和技术。</span><span class="sxs-lookup"><span data-stu-id="cd270-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="cd270-104">功能选择不仅意味着*基数降低*，还意味着对在生成模型时可以考虑的属性数量施加任意或预定义的截止时间，同时也意味着选择属性，这意味着分析师或建模工具会根据分析的有用性主动选择或放弃属性。</span><span class="sxs-lookup"><span data-stu-id="cd270-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="cd270-105">能够应用功能选择对于有效分析至关重要，因为数据集包含的信息通常多于生成模型所需的信息。</span><span class="sxs-lookup"><span data-stu-id="cd270-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="cd270-106">例如，一个数据集可能包含 500 个用来描述客户特征的列，但是，如果其中某些列的数据非常稀疏，则您从将这些数据添加到模型中所得到的利益可能会非常少。</span><span class="sxs-lookup"><span data-stu-id="cd270-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="cd270-107">如果在生成模型时保留不需要的列，则定型期间需要更多的 CPU 和内存，并且已完成的模型需要更多的存储空间。</span><span class="sxs-lookup"><span data-stu-id="cd270-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="cd270-108">即使资源不存在任何问题，由于不需要的列可能因为下列原因而降低发现的模式的质量，因此通常需要删除这些列：</span><span class="sxs-lookup"><span data-stu-id="cd270-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="cd270-109">某些列存在干扰或冗余。</span><span class="sxs-lookup"><span data-stu-id="cd270-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="cd270-110">此干扰会使从数据中发现有意义的模式更困难；</span><span class="sxs-lookup"><span data-stu-id="cd270-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="cd270-111">若要发现质量模式，大多数数据挖掘算法需要高维数据集上的较大定型数据集。</span><span class="sxs-lookup"><span data-stu-id="cd270-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="cd270-112">但是某些数据挖掘应用程序中的定型数据非常少。</span><span class="sxs-lookup"><span data-stu-id="cd270-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="cd270-113">在数据源的这 500 列中，如果只有 50 列具有在生成模型时有用的信息，则您可以只将这些列保持在模型之外，或者可以使用功能选择技术自动发现最佳功能并且排除在统计上无用的值。</span><span class="sxs-lookup"><span data-stu-id="cd270-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="cd270-114">功能选择有助于解决两个问题：无价值的数据过多或有价值的数据过少。</span><span class="sxs-lookup"><span data-stu-id="cd270-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="cd270-115">Analysis Services 数据挖掘中的功能选择</span><span class="sxs-lookup"><span data-stu-id="cd270-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="cd270-116">通常，功能选择在 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 中自动执行，并且每个算法都具有用于智能地应用功能选择的一组默认技术。</span><span class="sxs-lookup"><span data-stu-id="cd270-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="cd270-117">功能选择始终在对模型进行定型之前执行，以自动选择数据集中最有可能在该模型中使用的属性。</span><span class="sxs-lookup"><span data-stu-id="cd270-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="cd270-118">但是，您也可以手动设置参数以便影响功能选择行为。</span><span class="sxs-lookup"><span data-stu-id="cd270-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="cd270-119">通常，功能选择的工作方式是为每个属性计算一个分数，然后仅选择具有最高分数的属性。</span><span class="sxs-lookup"><span data-stu-id="cd270-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="cd270-120">您还可以调整最高分数的阈值。</span><span class="sxs-lookup"><span data-stu-id="cd270-120">You can also adjust the threshold for the top scores.</span></span> [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] <span data-ttu-id="cd270-121">提供多种方法来计算这些分数，适用于任何模型的准确方法取决于以下因素：</span><span class="sxs-lookup"><span data-stu-id="cd270-121">provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="cd270-122">在模型中使用的算法</span><span class="sxs-lookup"><span data-stu-id="cd270-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="cd270-123">属性的数据类型</span><span class="sxs-lookup"><span data-stu-id="cd270-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="cd270-124">可对模型设置的任何参数</span><span class="sxs-lookup"><span data-stu-id="cd270-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="cd270-125">功能选择应用于输入、可预测属性或列中的状态。</span><span class="sxs-lookup"><span data-stu-id="cd270-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="cd270-126">在用于功能选择的分数完整时，只有算法选择的属性和状态才会包含在模型生成过程中并可用于预测。</span><span class="sxs-lookup"><span data-stu-id="cd270-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="cd270-127">即使您选择的可预测属性未满足用于功能选择的阈值，这些属性仍可用于预测，但这些预测将只是基于模型中存在的全局统计信息进行的。</span><span class="sxs-lookup"><span data-stu-id="cd270-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="cd270-128">功能选择仅影响模型中使用的列，而对挖掘结构的存储没有任何影响。</span><span class="sxs-lookup"><span data-stu-id="cd270-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="cd270-129">留在挖掘模型之外的列仍在结构中可用，并将缓存挖掘结构列中的数据。</span><span class="sxs-lookup"><span data-stu-id="cd270-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="cd270-130">功能选择方法的定义</span><span class="sxs-lookup"><span data-stu-id="cd270-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="cd270-131">实现功能选择的方法很多，具体取决于使用的数据类型以及为分析选择的算法。</span><span class="sxs-lookup"><span data-stu-id="cd270-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="cd270-132">SQL Server Analysis Services 提供了用于对属性进行计分的若干现成的常用方法。</span><span class="sxs-lookup"><span data-stu-id="cd270-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="cd270-133">在任何算法或数据集中应用的方法取决于数据类型和列的用法。</span><span class="sxs-lookup"><span data-stu-id="cd270-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="cd270-134">“兴趣性 \*\* ”分数用于对包含非二进制连续数值数据的列中的属性进行排列和排序。</span><span class="sxs-lookup"><span data-stu-id="cd270-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="cd270-135">\*\* “Shannon 平均信息量”和两个“Bayesian” \*\* 分数可用于包含离散和离散化数据的列。</span><span class="sxs-lookup"><span data-stu-id="cd270-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="cd270-136">但是，如果模型包含任何连续列，则兴趣性分数将用于评估所有的输入列，以确保一致性。</span><span class="sxs-lookup"><span data-stu-id="cd270-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="cd270-137">下面的部分对功能选择的每种方法分别进行了介绍。</span><span class="sxs-lookup"><span data-stu-id="cd270-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="cd270-138">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="cd270-138">Interestingness score</span></span>  
 <span data-ttu-id="cd270-139">如果某个功能可以提供一些有用的信息，则该功能很令人感兴趣。</span><span class="sxs-lookup"><span data-stu-id="cd270-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="cd270-140">由于具体方案的定义很有用，具体取决于方案，数据挖掘行业开发了多种方法来度量*兴趣性*。</span><span class="sxs-lookup"><span data-stu-id="cd270-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="cd270-141">例如，*新奇*可能会在离群值检测中感兴趣，但在密切相关的项目或*对比权重*之间进行区分的能力可能更适合分类。</span><span class="sxs-lookup"><span data-stu-id="cd270-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="cd270-142">SQL Server Analysis Services 中使用的兴趣性的度量值是*基于平均信息量*的，这意味着具有随机分布的属性具有较高的平均信息量和较低的信息增益;因此，此类属性的意义不大。</span><span class="sxs-lookup"><span data-stu-id="cd270-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="cd270-143">任何特定属性的平均信息量都将与所有其他属性的平均信息量进行比较，如下所示：</span><span class="sxs-lookup"><span data-stu-id="cd270-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="cd270-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span><span class="sxs-lookup"><span data-stu-id="cd270-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="cd270-145">中央平均信息量（即 m）表示整个功能集的平均信息量。</span><span class="sxs-lookup"><span data-stu-id="cd270-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="cd270-146">通过用中央平均信息量减去目标属性的平均信息量，可以评估该属性提供了多少信息。</span><span class="sxs-lookup"><span data-stu-id="cd270-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="cd270-147">每当列包含非二进制连续数值数据时，就会默认使用此分数。</span><span class="sxs-lookup"><span data-stu-id="cd270-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="cd270-148">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="cd270-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="cd270-149">Shannon 平均信息量针对特定的结果度量随机变量的不确定性。</span><span class="sxs-lookup"><span data-stu-id="cd270-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="cd270-150">例如，抛硬币的平均信息量可以表示为其正面朝上概率的函数。</span><span class="sxs-lookup"><span data-stu-id="cd270-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="cd270-151">Analysis Services 使用以下公式来计算 Shannon 平均信息量：</span><span class="sxs-lookup"><span data-stu-id="cd270-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="cd270-152">H(X) = -  P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="cd270-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="cd270-153">此计分方法适用于离散和离散化的属性。</span><span class="sxs-lookup"><span data-stu-id="cd270-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="cd270-154">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="cd270-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="cd270-155">Analysis Services 提供了两种基于 Bayesian 网络的功能选择分数。</span><span class="sxs-lookup"><span data-stu-id="cd270-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="cd270-156">Bayesian 网络是状态的“定向 \*\* ”或“非循环 \*\* ”曲线图，也是状态间的转换，这意味着某些状态始终早于当前的状态，某些状态是较晚的，曲线图不会重复或循环。</span><span class="sxs-lookup"><span data-stu-id="cd270-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="cd270-157">根据定义，Bayesian 网络允许使用先前的知识。</span><span class="sxs-lookup"><span data-stu-id="cd270-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="cd270-158">但是，对于算法设计、性能和精确度而言，关于在以后状态的概率计算中要使用以前的哪一个状态的问题是很重要的。</span><span class="sxs-lookup"><span data-stu-id="cd270-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="cd270-159">从 Bayesian 网络中了解的 K2 算法是由 Cooper 和 Herskovits 开发的，经常在数据挖掘中使用。</span><span class="sxs-lookup"><span data-stu-id="cd270-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="cd270-160">该算法是可伸缩的，并可以分析多个变量，但需要对用作输入的变量进行排序。</span><span class="sxs-lookup"><span data-stu-id="cd270-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="cd270-161">有关详细信息，请参阅由 Chickering、Geiger 和 Heckerman 编写的 [了解 Bayesian 网络](https://go.microsoft.com/fwlink/?LinkId=105885) 。</span><span class="sxs-lookup"><span data-stu-id="cd270-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="cd270-162">此计分方法适用于离散和离散化的属性。</span><span class="sxs-lookup"><span data-stu-id="cd270-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="cd270-163">Bayesian Dirichlet Equivalent with Uniform Prior</span><span class="sxs-lookup"><span data-stu-id="cd270-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="cd270-164">Bayesian Dirichlet Equivalent (BDE) 分数还使用 Bayesian 分析来评估给定数据集的网络。</span><span class="sxs-lookup"><span data-stu-id="cd270-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="cd270-165">BDE 计分方法是由 Heckerman 开发的，并基于 Cooper 和 Herskovits 开发的 BD 指标。</span><span class="sxs-lookup"><span data-stu-id="cd270-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="cd270-166">Dirichlet 分布是描述网络中每个变量的条件概率的多项分布，其中很多属性对学习很有用。</span><span class="sxs-lookup"><span data-stu-id="cd270-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="cd270-167">Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) 方法假定一个 Dirichlet 分布的特殊事例，在这个事例中使用一个数学常量创建一个以前状态的固定或均匀分布。</span><span class="sxs-lookup"><span data-stu-id="cd270-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="cd270-168">BDE 分数还假定可能性均等，这意味着数据不应当用来区分相等的结构。</span><span class="sxs-lookup"><span data-stu-id="cd270-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="cd270-169">也就是说，如果 If A Then B 的分数与 If B Then A 的分数相同，则无法基于数据区分这两种结构，也无法推断因果关系。</span><span class="sxs-lookup"><span data-stu-id="cd270-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="cd270-170">有关 Bayesian 网络和这些计分方法实现的详细信息，请参阅 [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885)（了解 Bayesian 网络）。</span><span class="sxs-lookup"><span data-stu-id="cd270-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="cd270-171">Analysis Services 算法使用的功能选择方法</span><span class="sxs-lookup"><span data-stu-id="cd270-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="cd270-172">下表列出了支持功能选择的算法、该算法所使用的功能选择方法以及设置用于控制功能选择行为的参数：</span><span class="sxs-lookup"><span data-stu-id="cd270-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="cd270-173">算法</span><span class="sxs-lookup"><span data-stu-id="cd270-173">Algorithm</span></span>|<span data-ttu-id="cd270-174">分析方法</span><span class="sxs-lookup"><span data-stu-id="cd270-174">Method of analysis</span></span>|<span data-ttu-id="cd270-175">注释</span><span class="sxs-lookup"><span data-stu-id="cd270-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="cd270-176">Naive Bayes</span><span class="sxs-lookup"><span data-stu-id="cd270-176">Naive Bayes</span></span>|<span data-ttu-id="cd270-177">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="cd270-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="cd270-178">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="cd270-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="cd270-179">使用统一先验的 Bayesian Dirichlet（默认）</span><span class="sxs-lookup"><span data-stu-id="cd270-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="cd270-180">Microsoft Naïve Bayes 算法仅接受离散或离散化的属性，因此它不能使用兴趣性分数。</span><span class="sxs-lookup"><span data-stu-id="cd270-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="cd270-181">有关此算法的详细信息，请参阅 [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="cd270-182">决策树</span><span class="sxs-lookup"><span data-stu-id="cd270-182">Decision trees</span></span>|<span data-ttu-id="cd270-183">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="cd270-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="cd270-184">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="cd270-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="cd270-185">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="cd270-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="cd270-186">使用统一先验的 Bayesian Dirichlet（默认）</span><span class="sxs-lookup"><span data-stu-id="cd270-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="cd270-187">如果任何列包含非二进制连续值，则兴趣性分数将用于所有列，以确保一致性。</span><span class="sxs-lookup"><span data-stu-id="cd270-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="cd270-188">否则，将使用默认的功能选择方法或者您在创建模型时指定的方法。</span><span class="sxs-lookup"><span data-stu-id="cd270-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="cd270-189">有关此算法的详细信息，请参阅 [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="cd270-190">神经网络</span><span class="sxs-lookup"><span data-stu-id="cd270-190">Neural network</span></span>|<span data-ttu-id="cd270-191">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="cd270-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="cd270-192">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="cd270-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="cd270-193">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="cd270-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="cd270-194">使用统一先验的 Bayesian Dirichlet（默认）</span><span class="sxs-lookup"><span data-stu-id="cd270-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="cd270-195">只要数据包含连续列，Microsoft 神经网络算法就可同时使用基于平均信息量的方法和 Bayesian 方法。</span><span class="sxs-lookup"><span data-stu-id="cd270-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="cd270-196">有关此算法的详细信息，请参阅 [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="cd270-197">逻辑回归</span><span class="sxs-lookup"><span data-stu-id="cd270-197">Logistic regression</span></span>|<span data-ttu-id="cd270-198">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="cd270-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="cd270-199">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="cd270-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="cd270-200">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="cd270-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="cd270-201">使用统一先验的 Bayesian Dirichlet（默认）</span><span class="sxs-lookup"><span data-stu-id="cd270-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="cd270-202">虽然 Microsoft 逻辑回归算法基于 Microsoft 神经网络算法，但您不能自定义逻辑回归模型以控制功能选择行为；因此，功能选择始终默认为最适合该属性的方法。</span><span class="sxs-lookup"><span data-stu-id="cd270-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="cd270-203">如果所有属性均为离散或离散化属性，则默认值为 BDEU。</span><span class="sxs-lookup"><span data-stu-id="cd270-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="cd270-204">有关此算法的详细信息，请参阅 [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="cd270-205">群集</span><span class="sxs-lookup"><span data-stu-id="cd270-205">Clustering</span></span>|<span data-ttu-id="cd270-206">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="cd270-206">Interestingness score</span></span>|<span data-ttu-id="cd270-207">Microsoft 聚类分析算法可以使用离散或离散化数据。</span><span class="sxs-lookup"><span data-stu-id="cd270-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="cd270-208">但是，每个属性的分数仍将作为距离进行计算并且表示为连续数字，因此必须使用兴趣性分数。</span><span class="sxs-lookup"><span data-stu-id="cd270-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="cd270-209">有关此算法的详细信息，请参阅 [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="cd270-210">线性回归</span><span class="sxs-lookup"><span data-stu-id="cd270-210">Linear regression</span></span>|<span data-ttu-id="cd270-211">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="cd270-211">Interestingness score</span></span>|<span data-ttu-id="cd270-212">因为仅支持连续列，Microsoft 线性回归性算法只能使用兴趣性分数。</span><span class="sxs-lookup"><span data-stu-id="cd270-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="cd270-213">有关此算法的详细信息，请参阅 [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="cd270-214">关联规则</span><span class="sxs-lookup"><span data-stu-id="cd270-214">Association rules</span></span><br /><br /> <span data-ttu-id="cd270-215">顺序分析和聚类分析</span><span class="sxs-lookup"><span data-stu-id="cd270-215">Sequence clustering</span></span>|<span data-ttu-id="cd270-216">未使用</span><span class="sxs-lookup"><span data-stu-id="cd270-216">Not used</span></span>|<span data-ttu-id="cd270-217">不使用这些算法调用功能选择。</span><span class="sxs-lookup"><span data-stu-id="cd270-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="cd270-218">但在必要时，可以通过设置参数 MINIMUM_SUPPORT 和 MINIMUM_PROBABILIITY 的值来控制算法的行为和减小输入数据的大小。</span><span class="sxs-lookup"><span data-stu-id="cd270-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="cd270-219">有关详细信息，请参阅 [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) 和 [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="cd270-220">时序</span><span class="sxs-lookup"><span data-stu-id="cd270-220">Time series</span></span>|<span data-ttu-id="cd270-221">未使用</span><span class="sxs-lookup"><span data-stu-id="cd270-221">Not used</span></span>|<span data-ttu-id="cd270-222">功能选择不适用于时序模型。</span><span class="sxs-lookup"><span data-stu-id="cd270-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="cd270-223">有关此算法的详细信息，请参阅 [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="cd270-224">功能选择参数</span><span class="sxs-lookup"><span data-stu-id="cd270-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="cd270-225">在支持功能选择的算法中，可以使用下列参数来控制何时打开功能选择。</span><span class="sxs-lookup"><span data-stu-id="cd270-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="cd270-226">对于所允许的输入数目，每个算法都有一个默认值，但您可以覆盖此默认值并指定属性的数目。</span><span class="sxs-lookup"><span data-stu-id="cd270-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="cd270-227">本节列出了为管理功能选择而提供的参数。</span><span class="sxs-lookup"><span data-stu-id="cd270-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="cd270-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="cd270-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="cd270-229">如果某个模型包含的列多于 *MAXIMUM_INPUT_ATTRIBUTES* 参数中指定的数目，算法将忽略它计算后认为无用的任何列。</span><span class="sxs-lookup"><span data-stu-id="cd270-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="cd270-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="cd270-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="cd270-231">同样，如果某个模型包含的可预测列多于 *MAXIMUM_OUTPUT_ATTRIBUTES* 参数中指定的数目，算法将忽略它计算后认为无用的任何列。</span><span class="sxs-lookup"><span data-stu-id="cd270-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="cd270-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="cd270-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="cd270-233">如果某个模型包含的事例多于 *MAXIMUM_STATES* 参数中指定的数目，则最不常见的状态将被分组在一起且被视为不存在。</span><span class="sxs-lookup"><span data-stu-id="cd270-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="cd270-234">如果这些参数中的任何一个被设置为 0，则功能选择将被关闭，这会影响处理时间和性能。</span><span class="sxs-lookup"><span data-stu-id="cd270-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="cd270-235">除了上述用于功能选择的方法之外，您还可以通过对模型设置“建模标志” \*\* 或者对结构设置“分布标志” \*\* ，提高算法标识或改进有意义属性的能力。</span><span class="sxs-lookup"><span data-stu-id="cd270-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="cd270-236">有关这些概念的详细信息，请参阅[建模标志（数据挖掘）](modeling-flags-data-mining.md)和[列分布（数据挖掘）](column-distributions-data-mining.md)。</span><span class="sxs-lookup"><span data-stu-id="cd270-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="cd270-237">另请参阅</span><span class="sxs-lookup"><span data-stu-id="cd270-237">See Also</span></span>  
 [<span data-ttu-id="cd270-238">自定义挖掘模型和结构</span><span class="sxs-lookup"><span data-stu-id="cd270-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
