---
title: Microsoft 决策树算法技术参考 |Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- MAXIMUM_INPUT_ATTRIBUTES parameter
- SPLIT_METHOD parameter
- MINIMUM_SUPPORT parameter
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- FORCED_REGRESSOR parameter
- decision tree algorithms [Analysis Services]
- decision trees [Analysis Services]
- COMPLEXITY_PENALTY parameter
- SCORE_METHOD parameter
ms.assetid: 1e9f7969-0aa6-465a-b3ea-57b8d1c7a1fd
author: minewiskan
ms.author: owend
ms.openlocfilehash: 0cd0cd3100d0ed1213183815ae41f17cee3baa68
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 08/04/2020
ms.locfileid: "87588142"
---
# <a name="microsoft-decision-trees-algorithm-technical-reference"></a><span data-ttu-id="dc67c-102">Microsoft 决策树算法技术参考</span><span class="sxs-lookup"><span data-stu-id="dc67c-102">Microsoft Decision Trees Algorithm Technical Reference</span></span>
  <span data-ttu-id="dc67c-103">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法是一种混合算法，它综合了多种不同的创建树的方法，并支持多种分析任务，包括回归、分类以及关联。</span><span class="sxs-lookup"><span data-stu-id="dc67c-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm is a hybrid algorithm that incorporates different methods for creating a tree, and supports multiple analytic tasks, including regression, classification, and association.</span></span> <span data-ttu-id="dc67c-104">Microsoft 决策树算法支持对离散属性和连续属性进行建模。</span><span class="sxs-lookup"><span data-stu-id="dc67c-104">The Microsoft Decision Trees algorithm supports modeling of both discrete and continuous attributes.</span></span>  
  
 <span data-ttu-id="dc67c-105">本主题说明此算法的实现，介绍如何针对不同的任务自定义算法行为，并提供指向有关决策树模型查询的其他信息的链接。</span><span class="sxs-lookup"><span data-stu-id="dc67c-105">This topic explains the implementation of the algorithm, describes how to customize the behavior of the algorithm for different tasks, and provides links to additional information about querying decision tree models.</span></span>  
  
## <a name="implementation-of-the-decision-trees-algorithm"></a><span data-ttu-id="dc67c-106">决策树算法的实现</span><span class="sxs-lookup"><span data-stu-id="dc67c-106">Implementation of the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="dc67c-107">Microsoft 决策树算法通过获取模型的近似后验分布，将 Bayesian 方法应用于学习因果交互模型。</span><span class="sxs-lookup"><span data-stu-id="dc67c-107">The Microsoft Decision Trees algorithm applies the Bayesian approach to learning causal interaction models by obtaining approximate posterior distributions for the models.</span></span> <span data-ttu-id="dc67c-108">有关此方法的详细说明，请参阅 Microsoft Research 站点上的文章 [结构和参数学习](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-108">For a detailed explanation of this approach, see the paper on the Microsoft Research site, by [Structure and Parameter Learning](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409).</span></span>  
  
 <span data-ttu-id="dc67c-109">用于评估学习所需的“先验知识” \*\* 的信息值的方法基于“可能性均等” \*\* 假定。</span><span class="sxs-lookup"><span data-stu-id="dc67c-109">The methodology for assessing the information value of the *priors* needed for learning is based on the assumption of *likelihood equivalence*.</span></span> <span data-ttu-id="dc67c-110">该假定认为数据对区分以不同方式表示同一条件独立断言的网络结构没有帮助。</span><span class="sxs-lookup"><span data-stu-id="dc67c-110">This assumption says that data should not help to discriminate network structures that otherwise represent the same assertions of conditional independence.</span></span> <span data-ttu-id="dc67c-111">先假定每个事例都有一个 Bayesian 先验网络和一个网络置信度的度量值。</span><span class="sxs-lookup"><span data-stu-id="dc67c-111">Each case is assumed to have a single Bayesian prior network and a single measure of confidence for that network.</span></span>  
  
 <span data-ttu-id="dc67c-112">然后，算法会利用给定的当前定型数据，使用这些先验网络计算网络结构的相对“后验概率” \*\* ，并标识出具有最高后验概率的网络结构。</span><span class="sxs-lookup"><span data-stu-id="dc67c-112">Using these prior networks, the algorithm then computes the relative *posterior probabilities* of network structures given the current training data, and identifies the network structures that have the highest posterior probabilities.</span></span>  
  
 <span data-ttu-id="dc67c-113">Microsoft 决策树算法使用不同的方法来计算最佳的树。</span><span class="sxs-lookup"><span data-stu-id="dc67c-113">The Microsoft Decision Trees algorithm uses different methods to compute the best tree.</span></span> <span data-ttu-id="dc67c-114">所使用的方法具体取决于任务，任务可为线性回归、分类或关联分析。</span><span class="sxs-lookup"><span data-stu-id="dc67c-114">The method used depends on the task, which can be linear regression, classification, or association analysis.</span></span> <span data-ttu-id="dc67c-115">一个模型可包含多个针对不同可预测属性的树。</span><span class="sxs-lookup"><span data-stu-id="dc67c-115">A single model can contain multiple trees for different predictable attributes.</span></span> <span data-ttu-id="dc67c-116">而且，每个树可包含多个分支，具体取决于数据中包含的属性和值的数量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-116">Moreover, each tree can contain multiple branches, depending on how many attributes and values there are in the data.</span></span> <span data-ttu-id="dc67c-117">特定模型中生成的树的形状和深度取决于所使用的计分方法以及其他参数。</span><span class="sxs-lookup"><span data-stu-id="dc67c-117">The shape and depth of the tree built in a particular model depends on the scoring method and other parameters that were used.</span></span> <span data-ttu-id="dc67c-118">参数更改还会影响节点的拆分位置。</span><span class="sxs-lookup"><span data-stu-id="dc67c-118">Changes in the parameters can also affect where the nodes split.</span></span>  
  
### <a name="building-the-tree"></a><span data-ttu-id="dc67c-119">生成树</span><span class="sxs-lookup"><span data-stu-id="dc67c-119">Building the Tree</span></span>  
 <span data-ttu-id="dc67c-120">创建可能的输入值集时，Microsoft 决策树算法会执行 *feature selection* 来标识提供大部分信息的属性和值，而不会考虑非常少见的值。</span><span class="sxs-lookup"><span data-stu-id="dc67c-120">When the Microsoft Decision Trees algorithm creates the set of possible input values, it performs *feature selection* to identify the attributes and values that provide the most information, and removes from consideration the values that are very rare.</span></span> <span data-ttu-id="dc67c-121">该算法还会通过将值分组到“收集箱” \*\* 来创建值分组，这样的值分组可作为一个单元进行处理，从而使性能得到优化。</span><span class="sxs-lookup"><span data-stu-id="dc67c-121">The algorithm also groups values into *bins*, to create groupings of values that can be processed as a unit to optimize performance.</span></span>  
  
 <span data-ttu-id="dc67c-122">树是通过确定输入和目标结果之间的相关性而生成的。</span><span class="sxs-lookup"><span data-stu-id="dc67c-122">A tree is built by determining the correlations between an input and the targeted outcome.</span></span> <span data-ttu-id="dc67c-123">关联完所有属性后，算法会标识出最能完全分隔结果的属性。</span><span class="sxs-lookup"><span data-stu-id="dc67c-123">After all the attributes have been correlated, the algorithm identifies the single attribute that most cleanly separates the outcomes.</span></span> <span data-ttu-id="dc67c-124">最佳分隔点是通过使用计算信息获取分数的公式确定的。</span><span class="sxs-lookup"><span data-stu-id="dc67c-124">This point of the best separation is measured by using an equation that calculates information gain.</span></span> <span data-ttu-id="dc67c-125">信息获取分数最高的属性将用于将事例分为各个子集；然后，还会利用同一过程，以递归方式分析各子集，直至树无法拆分为止。</span><span class="sxs-lookup"><span data-stu-id="dc67c-125">The attribute that has the best score for information gain is used to divide the cases into subsets, which are then recursively analyzed by the same process, until the tree cannot be split any more.</span></span>  
  
 <span data-ttu-id="dc67c-126">用于计算信息获取分数的确切公式取决于创建算法时设置的参数、可预测列的数据类型以及输入的数据类型。</span><span class="sxs-lookup"><span data-stu-id="dc67c-126">The exact equation used to evaluate information gain depends on the parameters set when you created the algorithm, the data type of the predictable column, and the data type of the input.</span></span>  
  
### <a name="discrete-and-continuous-inputs"></a><span data-ttu-id="dc67c-127">离散输入和连续输入</span><span class="sxs-lookup"><span data-stu-id="dc67c-127">Discrete and Continuous Inputs</span></span>  
 <span data-ttu-id="dc67c-128">如果可预测属性和输入都是离散的，则计算每个输入对应的结果数量只涉及创建一个矩阵并为矩阵中的每个单元生成分数。</span><span class="sxs-lookup"><span data-stu-id="dc67c-128">When the predictable attribute is discrete and the inputs are discrete, counting the outcomes per input is a matter of creating a matrix and generating scores for each cell in the matrix.</span></span>  
  
 <span data-ttu-id="dc67c-129">但是，如果可预测属性是离散的，而输入是连续的，则会自动离散化连续的输入列。</span><span class="sxs-lookup"><span data-stu-id="dc67c-129">However, when the predictable attribute is discrete and the inputs are continuous, the input of the continuous columns are automatically discretized.</span></span> <span data-ttu-id="dc67c-130">你可以接受默认值，并让 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 查找最佳桶数，或者通过设置 <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> 和 <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> 属性来控制离散化连续输入的方式。</span><span class="sxs-lookup"><span data-stu-id="dc67c-130">You can accept the default and have [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] find the optimum number of bins, or you can control the manner in which continuous inputs are discretized by setting the <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> and <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> properties.</span></span> <span data-ttu-id="dc67c-131">有关详细信息，请参阅 [更改挖掘模型中列的离散化](change-the-discretization-of-a-column-in-a-mining-model.md)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-131">For more information, see [Change the Discretization of a Column in a Mining Model](change-the-discretization-of-a-column-in-a-mining-model.md).</span></span>  
  
 <span data-ttu-id="dc67c-132">对于连续属性，该算法使用线性回归确定决策树的拆分位置。</span><span class="sxs-lookup"><span data-stu-id="dc67c-132">For continuous attributes, the algorithm uses linear regression to determine where a decision tree splits.</span></span>  
  
 <span data-ttu-id="dc67c-133">如果可预测属性为连续数值数据类型，则还会对输出应用功能选择，以减少可能的结果数量，并更快地生成模型。</span><span class="sxs-lookup"><span data-stu-id="dc67c-133">When the predictable attribute is a continuous numeric data type, feature selection is applied to the outputs as well, to reduce the possible number of outcomes and build the model faster.</span></span> <span data-ttu-id="dc67c-134">您可以通过设置 MAXIMUM_OUTPUT_ATTRIBUTES 参数来更改功能选择的阈值，从而增加或减少可能值的数量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-134">You can change the threshold for feature selection and thereby increase or decrease the number of possible values by setting the MAXIMUM_OUTPUT_ATTRIBUTES parameter.</span></span>  
  
 <span data-ttu-id="dc67c-135">有关 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法如何处理离散可预测列的详细说明，请参阅 [学习 Bayesian 网络：知识与统计数据的组合](https://go.microsoft.com/fwlink/?LinkId=45963)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-135">For a more detained explanation about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with discrete predictable columns, see [Learning Bayesian Networks: The Combination of Knowledge and Statistical Data](https://go.microsoft.com/fwlink/?LinkId=45963).</span></span> <span data-ttu-id="dc67c-136">有关 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法如何处理可预测的连续列的详细信息，请参阅 [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966)（时序分析的自动回归树模型）的附录。</span><span class="sxs-lookup"><span data-stu-id="dc67c-136">For more information about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with a continuous predictable column, see the appendix of [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966).</span></span>  
  
### <a name="scoring-methods-and-feature-selection"></a><span data-ttu-id="dc67c-137">计分方法和功能选择</span><span class="sxs-lookup"><span data-stu-id="dc67c-137">Scoring Methods and Feature Selection</span></span>  
 <span data-ttu-id="dc67c-138">Microsoft 决策树算法提供了三种信息获取计分公式：Shannon 平均信息量、使用 K2 先验的 Bayesian 网络和使用先验统一 Dirichlet 分布的 Bayesian 网络。</span><span class="sxs-lookup"><span data-stu-id="dc67c-138">The Microsoft Decision Trees algorithm offers three formulas for scoring information gain: Shannon's entropy, Bayesian network with K2 prior, and Bayesian network with a uniform Dirichlet distribution of priors.</span></span> <span data-ttu-id="dc67c-139">这三种都是数据挖掘领域中已经确立的方法。</span><span class="sxs-lookup"><span data-stu-id="dc67c-139">All three methods are well established in the data mining field.</span></span> <span data-ttu-id="dc67c-140">建议您利用不同的参数，分别试用这些方法，以确定哪种方法结果最佳。</span><span class="sxs-lookup"><span data-stu-id="dc67c-140">We recommend that you experiment with different parameters and scoring methods to determine which provides the best results.</span></span> <span data-ttu-id="dc67c-141">有关这些计分方法的详细信息，请参阅 [Feature Selection](../../sql-server/install/feature-selection.md)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-141">For more information about these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="dc67c-142">所有 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 数据挖掘算法都会自动使用功能选择来改善分析效果和减轻处理工作量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-142">All [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms automatically use feature selection to improve analysis and reduce processing load.</span></span> <span data-ttu-id="dc67c-143">用于功能选择的方法取决于生成模型所用的算法。</span><span class="sxs-lookup"><span data-stu-id="dc67c-143">The method used for feature selection depends on the algorithm that is used to build the model.</span></span> <span data-ttu-id="dc67c-144">控制决策树模型的功能选择的算法参数为 MAXIMUM_INPUT_ATTRIBUTES 和 MAXIMUM_OUTPUT。</span><span class="sxs-lookup"><span data-stu-id="dc67c-144">The algorithm parameters that control feature selection for a decision trees model are MAXIMUM_INPUT_ATTRIBUTES and MAXIMUM_OUTPUT.</span></span>  
  
|<span data-ttu-id="dc67c-145">算法</span><span class="sxs-lookup"><span data-stu-id="dc67c-145">Algorithm</span></span>|<span data-ttu-id="dc67c-146">分析方法</span><span class="sxs-lookup"><span data-stu-id="dc67c-146">Method of analysis</span></span>|<span data-ttu-id="dc67c-147">注释</span><span class="sxs-lookup"><span data-stu-id="dc67c-147">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="dc67c-148">决策树</span><span class="sxs-lookup"><span data-stu-id="dc67c-148">Decision Trees</span></span>|<span data-ttu-id="dc67c-149">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="dc67c-149">Interestingness score</span></span><br /><br /> <span data-ttu-id="dc67c-150">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="dc67c-150">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="dc67c-151">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="dc67c-151">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="dc67c-152">使用统一先验的 Bayesian Dirichlet（默认）</span><span class="sxs-lookup"><span data-stu-id="dc67c-152">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="dc67c-153">如果任何列包含非二进制连续值，则兴趣性分数将用于所有列，以确保一致性。</span><span class="sxs-lookup"><span data-stu-id="dc67c-153">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="dc67c-154">否则，将使用默认方法或指定的方法。</span><span class="sxs-lookup"><span data-stu-id="dc67c-154">Otherwise, the default or specified method is used.</span></span>|  
|<span data-ttu-id="dc67c-155">线性回归</span><span class="sxs-lookup"><span data-stu-id="dc67c-155">Linear Regression</span></span>|<span data-ttu-id="dc67c-156">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="dc67c-156">Interestingness score</span></span>|<span data-ttu-id="dc67c-157">线形回归仅使用兴趣性分数，原因是它仅支持连续列。</span><span class="sxs-lookup"><span data-stu-id="dc67c-157">Linear Regression only uses interestingness, because it only supports continuous columns.</span></span>|  
  
### <a name="scalability-and-performance"></a><span data-ttu-id="dc67c-158">可伸缩性和性能</span><span class="sxs-lookup"><span data-stu-id="dc67c-158">Scalability and Performance</span></span>  
 <span data-ttu-id="dc67c-159">分类是一种重要的数据挖掘策略。</span><span class="sxs-lookup"><span data-stu-id="dc67c-159">Classification is an important data mining strategy.</span></span> <span data-ttu-id="dc67c-160">通常，分类事例所需的信息量同输入记录的数量成正比例增长。</span><span class="sxs-lookup"><span data-stu-id="dc67c-160">Generally, the amount of information that is needed to classify the cases grows in direct proportion to the number of input records.</span></span> <span data-ttu-id="dc67c-161">这将会限制可进行分类的数据量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-161">This limits the size of the data that can be classified.</span></span> <span data-ttu-id="dc67c-162">Microsoft 决策树算法使用以下方法来解决这些问题、提高性能并消除内存限制：</span><span class="sxs-lookup"><span data-stu-id="dc67c-162">The Microsoft Decision Trees algorithm using uses the following methods to resolve these problems, improve performance, and eliminate memory restrictions:</span></span>  
  
-   <span data-ttu-id="dc67c-163">使用功能选择优化属性的选择。</span><span class="sxs-lookup"><span data-stu-id="dc67c-163">Feature selection to optimize the selection of attributes.</span></span>  
  
-   <span data-ttu-id="dc67c-164">使用 Bayesian 计分控制树的增长。</span><span class="sxs-lookup"><span data-stu-id="dc67c-164">Bayesian scoring to control tree growth.</span></span>  
  
-   <span data-ttu-id="dc67c-165">优化连续属性的收集。</span><span class="sxs-lookup"><span data-stu-id="dc67c-165">Optimization of binning for continuous attributes.</span></span>  
  
-   <span data-ttu-id="dc67c-166">动态分组输入值以确定最重要的值。</span><span class="sxs-lookup"><span data-stu-id="dc67c-166">Dynamic grouping of input values to determine the most important values.</span></span>  
  
 <span data-ttu-id="dc67c-167">Microsoft 决策树算法高效快速且可伸缩，可轻松实现并行化，这意味着所有处理器均可协同工作，共同生成一个一致的模型。</span><span class="sxs-lookup"><span data-stu-id="dc67c-167">The Microsoft Decision Trees algorithm is fast and scalable, and has been designed to be easily parallelized, meaning that all processors work together to build a single, consistent model.</span></span> <span data-ttu-id="dc67c-168">这些特征使决策树分类器成为了理想的数据挖掘工具。</span><span class="sxs-lookup"><span data-stu-id="dc67c-168">The combination of these characteristics makes the decision-tree classifier an ideal tool for data mining.</span></span>  
  
 <span data-ttu-id="dc67c-169">如果性能约束比较严格，则您可以使用以下方法来缩短决策树模型定型过程中的处理时间。</span><span class="sxs-lookup"><span data-stu-id="dc67c-169">If performance constraints are severe, you might be able to improve processing time during the training of a decision tree model by using the following methods.</span></span> <span data-ttu-id="dc67c-170">如果使用这些方法，请注意：通过消除属性来改善处理性能将会使模型的结果发生变化，可能无法很好地显示总体情况。</span><span class="sxs-lookup"><span data-stu-id="dc67c-170">However, if you do so, be aware that eliminating attributes to improve processing performance will change the results of the model, and possibly make it less representative of the total population.</span></span>  
  
-   <span data-ttu-id="dc67c-171">增大 COMPLEXITY_PENALTY 参数的值以限制树的增长。</span><span class="sxs-lookup"><span data-stu-id="dc67c-171">Increase the value of the COMPLEXITY_PENALTY parameter to limit tree growth.</span></span>  
  
-   <span data-ttu-id="dc67c-172">限制关联模型中的项数以限制生成的树的数量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-172">Limit the number of items in association models to limit the number of trees that are built.</span></span>  
  
-   <span data-ttu-id="dc67c-173">增大 MINIMUM_SUPPORT 参数的值以避免过度拟合。</span><span class="sxs-lookup"><span data-stu-id="dc67c-173">Increase the value of the MINIMUM_SUPPORT parameter to avoid overfitting.</span></span>  
  
-   <span data-ttu-id="dc67c-174">将所有属性的离散值的数量限制为 10 或更小。</span><span class="sxs-lookup"><span data-stu-id="dc67c-174">Restrict the number of discrete values for any attribute to 10 or less.</span></span> <span data-ttu-id="dc67c-175">您可尝试以不同的方式，对不同模型中的值进行分组。</span><span class="sxs-lookup"><span data-stu-id="dc67c-175">You might try grouping values in different ways in different models.</span></span>  
  
    > [!NOTE]  
    >  <span data-ttu-id="dc67c-176">您可以使用  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] 中提供的数据浏览工具，在进行数据挖掘之前，先对数据中的值的分布进行可视化处理，并对这些值进行适当地分组。</span><span class="sxs-lookup"><span data-stu-id="dc67c-176">You can use the data exploration tools available in  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] to visualize the distribution of values in your data and group your values appropriately before beginning data mining.</span></span> <span data-ttu-id="dc67c-177">有关详细信息，请参阅 [数据事件探查任务和查看器](../../integration-services/control-flow/data-profiling-task-and-viewer.md)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-177">For more information, see [Data Profiling Task and Viewer](../../integration-services/control-flow/data-profiling-task-and-viewer.md).</span></span> <span data-ttu-id="dc67c-178">你还可以使用 [Excel 2007 数据挖掘外接程序](https://www.microsoft.com/download/details.aspx?id=8569)，在 Microsoft Excel 中浏览、分组和重新标记数据。</span><span class="sxs-lookup"><span data-stu-id="dc67c-178">You can also use the [Data Mining Add-ins for Excel 2007](https://www.microsoft.com/download/details.aspx?id=8569), to explore, group and relabel data in Microsoft Excel.</span></span>  
  
## <a name="customizing-the-decision-trees-algorithm"></a><span data-ttu-id="dc67c-179">自定义决策树算法</span><span class="sxs-lookup"><span data-stu-id="dc67c-179">Customizing the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="dc67c-180">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法支持多个参数，这些参数可影响所生成的挖掘模型的性能和准确性。</span><span class="sxs-lookup"><span data-stu-id="dc67c-180">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports parameters that affect the performance and accuracy of the resulting mining model.</span></span> <span data-ttu-id="dc67c-181">您还可以对挖掘模型列或挖掘结构列设置建模标志来控制数据的处理方式。</span><span class="sxs-lookup"><span data-stu-id="dc67c-181">You can also set modeling flags on the mining model columns or mining structure columns to control the way that data is processed.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="dc67c-182">在所有版本的 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]中均提供 Microsoft 决策树算法；但是，用于自定义 Microsoft 决策树算法行为的某些高级参数仅在 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]的特定版本中提供。</span><span class="sxs-lookup"><span data-stu-id="dc67c-182">The Microsoft Decision Trees algorithm is available in all editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]; however, some advanced parameters for customizing the behavior of the Microsoft Decision Trees algorithm are available for use only in specific editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)].</span></span> <span data-ttu-id="dc67c-183">有关各个版本支持的功能列表 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ，请参阅[SQL Server 2012 (版本支持的功能](https://go.microsoft.com/fwlink/?linkid=232473) https://go.microsoft.com/fwlink/?linkid=232473) 。</span><span class="sxs-lookup"><span data-stu-id="dc67c-183">For a list of features that are supported by the editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)], see [Features Supported by the Editions of SQL Server 2012](https://go.microsoft.com/fwlink/?linkid=232473) (https://go.microsoft.com/fwlink/?linkid=232473).</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="dc67c-184">设置算法参数</span><span class="sxs-lookup"><span data-stu-id="dc67c-184">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="dc67c-185">下表介绍了可用于 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法的参数。</span><span class="sxs-lookup"><span data-stu-id="dc67c-185">The following table describes the parameters that you can use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm.</span></span>  
  
 <span data-ttu-id="dc67c-186">*COMPLEXITY_PENALTY*</span><span class="sxs-lookup"><span data-stu-id="dc67c-186">*COMPLEXITY_PENALTY*</span></span>  
 <span data-ttu-id="dc67c-187">控制决策树的增长。</span><span class="sxs-lookup"><span data-stu-id="dc67c-187">Controls the growth of the decision tree.</span></span> <span data-ttu-id="dc67c-188">该值较低时，会增加拆分数；该值较高时，会减少拆分数。</span><span class="sxs-lookup"><span data-stu-id="dc67c-188">A low value increases the number of splits, and a high value decreases the number of splits.</span></span> <span data-ttu-id="dc67c-189">默认值基于特定模型的属性数，详见以下列表：</span><span class="sxs-lookup"><span data-stu-id="dc67c-189">The default value is based on the number of attributes for a particular model, as described in the following list:</span></span>  
  
-   <span data-ttu-id="dc67c-190">对于 1 到 9 个属性，默认值为 0.5。</span><span class="sxs-lookup"><span data-stu-id="dc67c-190">For 1 through 9 attributes, the default is 0.5.</span></span>  
  
-   <span data-ttu-id="dc67c-191">对于 10 到 99 个属性，默认值为 0.9。</span><span class="sxs-lookup"><span data-stu-id="dc67c-191">For 10 through 99 attributes, the default is 0.9.</span></span>  
  
-   <span data-ttu-id="dc67c-192">对于 100 或更多个属性，默认值为 0.99。</span><span class="sxs-lookup"><span data-stu-id="dc67c-192">For 100 or more attributes, the default is 0.99.</span></span>  
  
 <span data-ttu-id="dc67c-193">*FORCE_REGRESSOR*</span><span class="sxs-lookup"><span data-stu-id="dc67c-193">*FORCE_REGRESSOR*</span></span>  
 <span data-ttu-id="dc67c-194">强制算法将指定的列用作回归量，而不考虑算法计算出的列的重要性。</span><span class="sxs-lookup"><span data-stu-id="dc67c-194">Forces the algorithm to use the specified columns as regressors, regardless of the importance of the columns as calculated by the algorithm.</span></span> <span data-ttu-id="dc67c-195">此参数只用于预测连续属性的决策树。</span><span class="sxs-lookup"><span data-stu-id="dc67c-195">This parameter is only used for decision trees that are predicting a continuous attribute.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="dc67c-196">通过设置此参数，您可以强制要求算法尝试将属性用作回归量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-196">By setting this parameter, you force the algorithm to try to use the attribute as a regressor.</span></span> <span data-ttu-id="dc67c-197">但是，属性实际是否会在最终模型中用作回归量取决于分析结果。</span><span class="sxs-lookup"><span data-stu-id="dc67c-197">However, whether the attribute is actually used as a regressor in the final model depends on the results of analysis.</span></span> <span data-ttu-id="dc67c-198">您可以通过查询模型内容来确定用作了回归量的列。</span><span class="sxs-lookup"><span data-stu-id="dc67c-198">You can find out which columns were used as regressors by querying the model content.</span></span>  
  
 <span data-ttu-id="dc67c-199">[仅可用于某些版本的 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span><span class="sxs-lookup"><span data-stu-id="dc67c-199">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span></span>  
  
 <span data-ttu-id="dc67c-200">*MAXIMUM_INPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="dc67c-200">*MAXIMUM_INPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="dc67c-201">定义算法在调用功能选择之前可以处理的输入属性数。</span><span class="sxs-lookup"><span data-stu-id="dc67c-201">Defines the number of input attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="dc67c-202">默认值为 255。</span><span class="sxs-lookup"><span data-stu-id="dc67c-202">The default is 255.</span></span>  
  
 <span data-ttu-id="dc67c-203">如果将此值设置为 0，则表示关闭功能选择。</span><span class="sxs-lookup"><span data-stu-id="dc67c-203">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="dc67c-204">[仅可用于某些版本的 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span><span class="sxs-lookup"><span data-stu-id="dc67c-204">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="dc67c-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="dc67c-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="dc67c-206">定义算法在调用功能选择之前可以处理的输出属性数。</span><span class="sxs-lookup"><span data-stu-id="dc67c-206">Defines the number of output attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="dc67c-207">默认值为 255。</span><span class="sxs-lookup"><span data-stu-id="dc67c-207">The default is 255.</span></span>  
  
 <span data-ttu-id="dc67c-208">如果将此值设置为 0，则表示关闭功能选择。</span><span class="sxs-lookup"><span data-stu-id="dc67c-208">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="dc67c-209">[仅可用于某些版本的 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span><span class="sxs-lookup"><span data-stu-id="dc67c-209">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="dc67c-210">*MINIMUM_SUPPORT*</span><span class="sxs-lookup"><span data-stu-id="dc67c-210">*MINIMUM_SUPPORT*</span></span>  
 <span data-ttu-id="dc67c-211">确定在决策树中生成拆分所需的叶事例的最少数量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-211">Determines the minimum number of leaf cases that is required to generate a split in the decision tree.</span></span>  
  
 <span data-ttu-id="dc67c-212">默认值为 10。</span><span class="sxs-lookup"><span data-stu-id="dc67c-212">The default is 10.</span></span>  
  
 <span data-ttu-id="dc67c-213">如果数据集非常大，则可能需要增大此值，以避免过度定型。</span><span class="sxs-lookup"><span data-stu-id="dc67c-213">You may need to increase this value if the dataset is very large, to avoid overtraining.</span></span>  
  
 <span data-ttu-id="dc67c-214">*SCORE_METHOD*</span><span class="sxs-lookup"><span data-stu-id="dc67c-214">*SCORE_METHOD*</span></span>  
 <span data-ttu-id="dc67c-215">确定用于计算拆分分数的方法。</span><span class="sxs-lookup"><span data-stu-id="dc67c-215">Determines the method that is used to calculate the split score.</span></span> <span data-ttu-id="dc67c-216">提供了以下选项：</span><span class="sxs-lookup"><span data-stu-id="dc67c-216">The following options are available:</span></span>  
  
|<span data-ttu-id="dc67c-217">ID</span><span class="sxs-lookup"><span data-stu-id="dc67c-217">ID</span></span>|<span data-ttu-id="dc67c-218">名称</span><span class="sxs-lookup"><span data-stu-id="dc67c-218">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="dc67c-219">1</span><span class="sxs-lookup"><span data-stu-id="dc67c-219">1</span></span>|<span data-ttu-id="dc67c-220">Entropy</span><span class="sxs-lookup"><span data-stu-id="dc67c-220">Entropy</span></span>|  
|<span data-ttu-id="dc67c-221">3</span><span class="sxs-lookup"><span data-stu-id="dc67c-221">3</span></span>|<span data-ttu-id="dc67c-222">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="dc67c-222">Bayesian with K2 Prior</span></span>|  
|<span data-ttu-id="dc67c-223">4</span><span class="sxs-lookup"><span data-stu-id="dc67c-223">4</span></span>|<span data-ttu-id="dc67c-224">Bayesian Dirichlet Equivalent (BDE) with uniform prior</span><span class="sxs-lookup"><span data-stu-id="dc67c-224">Bayesian Dirichlet Equivalent (BDE) with uniform prior</span></span><br /><br /> <span data-ttu-id="dc67c-225">（默认值）</span><span class="sxs-lookup"><span data-stu-id="dc67c-225">(default)</span></span>|  
  
 <span data-ttu-id="dc67c-226">默认值为 4 或 BDE。</span><span class="sxs-lookup"><span data-stu-id="dc67c-226">The default is 4, or BDE.</span></span>  
  
 <span data-ttu-id="dc67c-227">有关这些计分方法的说明，请参阅 [Feature Selection](../../sql-server/install/feature-selection.md)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-227">For an explanation of these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="dc67c-228">*SPLIT_METHOD*</span><span class="sxs-lookup"><span data-stu-id="dc67c-228">*SPLIT_METHOD*</span></span>  
 <span data-ttu-id="dc67c-229">确定用于拆分节点的方法。</span><span class="sxs-lookup"><span data-stu-id="dc67c-229">Determines the method that is used to split the node.</span></span> <span data-ttu-id="dc67c-230">提供了以下选项：</span><span class="sxs-lookup"><span data-stu-id="dc67c-230">The following options are available:</span></span>  
  
|<span data-ttu-id="dc67c-231">ID</span><span class="sxs-lookup"><span data-stu-id="dc67c-231">ID</span></span>|<span data-ttu-id="dc67c-232">名称</span><span class="sxs-lookup"><span data-stu-id="dc67c-232">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="dc67c-233">1</span><span class="sxs-lookup"><span data-stu-id="dc67c-233">1</span></span>|<span data-ttu-id="dc67c-234">**Binary:** 指示无论属性值的实际数量是多少，树都拆分为两个分支。</span><span class="sxs-lookup"><span data-stu-id="dc67c-234">**Binary:** Indicates that regardless of the actual number of values for the attribute, the tree should be split into two branches.</span></span>|  
|<span data-ttu-id="dc67c-235">2</span><span class="sxs-lookup"><span data-stu-id="dc67c-235">2</span></span>|<span data-ttu-id="dc67c-236">**Complete:** 指示树可以创建与属性值数目相同的分叉。</span><span class="sxs-lookup"><span data-stu-id="dc67c-236">**Complete:** Indicates that the tree can create as many splits as there are attribute values.</span></span>|  
|<span data-ttu-id="dc67c-237">3</span><span class="sxs-lookup"><span data-stu-id="dc67c-237">3</span></span>|<span data-ttu-id="dc67c-238">**Both:** 指定 Analysis Services 可确定应使用 binary 还是 complete，以获得最佳结果。</span><span class="sxs-lookup"><span data-stu-id="dc67c-238">**Both:** Specifies that Analysis Services can determine whether a binary or complete split should be used to produce the best results.</span></span>|  
  
 <span data-ttu-id="dc67c-239">默认值为 3。</span><span class="sxs-lookup"><span data-stu-id="dc67c-239">The default is 3.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="dc67c-240">建模标志</span><span class="sxs-lookup"><span data-stu-id="dc67c-240">Modeling Flags</span></span>  
 <span data-ttu-id="dc67c-241">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法支持下列建模标志。</span><span class="sxs-lookup"><span data-stu-id="dc67c-241">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the following modeling flags.</span></span> <span data-ttu-id="dc67c-242">创建挖掘结构或挖掘模型时，定义建模标志以指定分析期间如何处理每列中的值。</span><span class="sxs-lookup"><span data-stu-id="dc67c-242">When you create the mining structure or mining model, you define modeling flags to specify how values in each column are handled during analysis.</span></span> <span data-ttu-id="dc67c-243">有关详细信息，请参阅[建模标志（数据挖掘）](modeling-flags-data-mining.md)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-243">For more information, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md).</span></span>  
  
|<span data-ttu-id="dc67c-244">建模标志</span><span class="sxs-lookup"><span data-stu-id="dc67c-244">Modeling Flag</span></span>|<span data-ttu-id="dc67c-245">说明</span><span class="sxs-lookup"><span data-stu-id="dc67c-245">Description</span></span>|  
|-------------------|-----------------|  
|<span data-ttu-id="dc67c-246">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="dc67c-246">MODEL_EXISTENCE_ONLY</span></span>|<span data-ttu-id="dc67c-247">表示列将被视为具有两个可能状态：`Missing` 和 `Existing`。</span><span class="sxs-lookup"><span data-stu-id="dc67c-247">Means that the column will be treated as having two possible states: `Missing` and `Existing`.</span></span> <span data-ttu-id="dc67c-248">Null 表示缺失值。</span><span class="sxs-lookup"><span data-stu-id="dc67c-248">A null is a missing value.</span></span><br /><br /> <span data-ttu-id="dc67c-249">适用于挖掘模型列。</span><span class="sxs-lookup"><span data-stu-id="dc67c-249">Applies to mining model columns.</span></span>|  
|<span data-ttu-id="dc67c-250">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="dc67c-250">NOT NULL</span></span>|<span data-ttu-id="dc67c-251">指示该列不能包含 Null。</span><span class="sxs-lookup"><span data-stu-id="dc67c-251">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="dc67c-252">如果 Analysis Services 在模型定型过程中遇到 Null 值，将会导致错误。</span><span class="sxs-lookup"><span data-stu-id="dc67c-252">An error will result if Analysis Services encounters a null during model training.</span></span><br /><br /> <span data-ttu-id="dc67c-253">适用于挖掘结构列。</span><span class="sxs-lookup"><span data-stu-id="dc67c-253">Applies to mining structure columns.</span></span>|  
  
### <a name="regressors-in-decision-tree-models"></a><span data-ttu-id="dc67c-254">决策树模型中的回归量</span><span class="sxs-lookup"><span data-stu-id="dc67c-254">Regressors in Decision Tree Models</span></span>  
 <span data-ttu-id="dc67c-255">即使不使用 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 线性回归算法，所有其输入和输出均为连续数值的决策树模型都可能会包含表示连续属性的回归的节点。</span><span class="sxs-lookup"><span data-stu-id="dc67c-255">Even if you do not use the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithm, any decision tree model that has continuous numeric inputs and outputs can potentially include nodes that represent a regression on a continuous attribute.</span></span>  
  
 <span data-ttu-id="dc67c-256">您无需指定连续数值数据列表示回归量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-256">You do not need to specify that a column of continuous numeric data represents a regressor.</span></span> <span data-ttu-id="dc67c-257">即使不对列设置 REGRESSOR 标志， [!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法也会自动将列用作潜在回归量，并会将数据集分区成具有一定意义的模式的区域。</span><span class="sxs-lookup"><span data-stu-id="dc67c-257">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm will automatically use the column as a potential regressor and partition the dataset into regions with meaningful patterns even if you do not set the REGRESSOR flag on the column.</span></span>  
  
 <span data-ttu-id="dc67c-258">您可以使用 FORCE_REGRESSOR 参数来确保算法将使用某一特定回归量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-258">However, you can use the FORCE_REGRESSOR parameter to guarantee that the algorithm will use a particular regressor.</span></span> <span data-ttu-id="dc67c-259">此参数只可用于 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法和 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 线性回归算法。</span><span class="sxs-lookup"><span data-stu-id="dc67c-259">This parameter can be used only with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees and [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithms.</span></span> <span data-ttu-id="dc67c-260">设置建模标志时，算法会尝试查找窗体 a \* C1 + b \* C2 + .。。以适应树节点中的模式。</span><span class="sxs-lookup"><span data-stu-id="dc67c-260">When you set the modeling flag, the algorithm will try to find regression equations of the form a\*C1 + b\*C2 + ... to fit the patterns in the nodes of the tree.</span></span> <span data-ttu-id="dc67c-261">将对剩余的总和进行计算，如果偏差过大，则在树中执行强制拆分。</span><span class="sxs-lookup"><span data-stu-id="dc67c-261">The sum of the residuals is calculated, and if the deviation is too great, a split is forced in the tree.</span></span>  
  
 <span data-ttu-id="dc67c-262">例如，如果要将 **Income** 用作属性来预测客户的购买行为，并对列设置 REGRESSOR 建模标志，则算法将会先通过使用标准回归公式来尝试拟合 **Income** 值。</span><span class="sxs-lookup"><span data-stu-id="dc67c-262">For example, if you are predicting customer purchasing behavior using **Income** as an attribute, and set the REGRESSOR modeling flag on the column, the algorithm will first try to fit the **Income** values by using a standard regression formula.</span></span> <span data-ttu-id="dc67c-263">如果偏差过大，则会放弃回归公式，并根据其他属性对树进行拆分。</span><span class="sxs-lookup"><span data-stu-id="dc67c-263">If the deviation is too great, the regression formula is abandoned and the tree will be split on another attribute.</span></span> <span data-ttu-id="dc67c-264">拆分完毕后，决策树算法将尝试拟合每个分支中的 Income 的回归量。</span><span class="sxs-lookup"><span data-stu-id="dc67c-264">The decision tree algorithm will then try to fit a regressor for income in each of the branches after the split.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="dc67c-265">要求</span><span class="sxs-lookup"><span data-stu-id="dc67c-265">Requirements</span></span>  
 <span data-ttu-id="dc67c-266">一个决策树模型必须包含一个键列、若干输入列和至少一个可预测列。</span><span class="sxs-lookup"><span data-stu-id="dc67c-266">A decision tree model must contain a key column, input columns, and at least one predictable column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="dc67c-267">输入列和可预测列</span><span class="sxs-lookup"><span data-stu-id="dc67c-267">Input and Predictable Columns</span></span>  
 <span data-ttu-id="dc67c-268">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 决策树算法支持下表中列出的特定输入列和可预测列。</span><span class="sxs-lookup"><span data-stu-id="dc67c-268">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span> <span data-ttu-id="dc67c-269">有关内容类型在用于挖掘模型中时的含义的详细信息，请参阅[内容类型（数据挖掘）](content-types-data-mining.md)。</span><span class="sxs-lookup"><span data-stu-id="dc67c-269">For more information about what the content types mean when used in a mining model, see [Content Types &#40;Data Mining&#41;](content-types-data-mining.md).</span></span>  
  
|<span data-ttu-id="dc67c-270">列</span><span class="sxs-lookup"><span data-stu-id="dc67c-270">Column</span></span>|<span data-ttu-id="dc67c-271">内容类型</span><span class="sxs-lookup"><span data-stu-id="dc67c-271">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="dc67c-272">输入属性</span><span class="sxs-lookup"><span data-stu-id="dc67c-272">Input attribute</span></span>|<span data-ttu-id="dc67c-273">Continuous、Cyclical、Discrete、Discretized、Key、Ordered 和 Table</span><span class="sxs-lookup"><span data-stu-id="dc67c-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span></span>|  
|<span data-ttu-id="dc67c-274">可预测属性</span><span class="sxs-lookup"><span data-stu-id="dc67c-274">Predictable attribute</span></span>|<span data-ttu-id="dc67c-275">Continuous、Cyclical、Discrete、Discretized、Ordered 和 Table</span><span class="sxs-lookup"><span data-stu-id="dc67c-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="dc67c-276">支持 Cyclical 和 Ordered 内容类型，但算法会将它们视为离散值，不会进行特殊处理。</span><span class="sxs-lookup"><span data-stu-id="dc67c-276">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="dc67c-277">另请参阅</span><span class="sxs-lookup"><span data-stu-id="dc67c-277">See Also</span></span>  
 <span data-ttu-id="dc67c-278">[Microsoft 决策树算法](microsoft-decision-trees-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="dc67c-278">[Microsoft Decision Trees Algorithm](microsoft-decision-trees-algorithm.md) </span></span>  
 <span data-ttu-id="dc67c-279">[决策树模型查询示例](decision-trees-model-query-examples.md) </span><span class="sxs-lookup"><span data-stu-id="dc67c-279">[Decision Trees Model Query Examples](decision-trees-model-query-examples.md) </span></span>  
 [<span data-ttu-id="dc67c-280">决策树模型的挖掘模型内容（Analysis Services - 数据挖掘）</span><span class="sxs-lookup"><span data-stu-id="dc67c-280">Mining Model Content for Decision Tree Models &#40;Analysis Services - Data Mining&#41;</span></span>](mining-model-content-for-decision-tree-models-analysis-services-data-mining.md)  
  
  
