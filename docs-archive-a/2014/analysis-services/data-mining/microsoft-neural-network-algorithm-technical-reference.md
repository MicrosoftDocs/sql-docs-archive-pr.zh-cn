---
title: Microsoft 神经网络算法技术参考 |Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 08/04/2020
ms.locfileid: "87577092"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="f65c7-102">Microsoft 神经网络算法技术参考</span><span class="sxs-lookup"><span data-stu-id="f65c7-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="f65c7-103">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 神经网络使用由最多三层神经元（即感知器**）组成的“多层感知器”** 网络（也称为“反向传播 Delta 法则”网络\*\*）。</span><span class="sxs-lookup"><span data-stu-id="f65c7-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="f65c7-104">这些层分别是输入层、可选隐藏层和输出层。</span><span class="sxs-lookup"><span data-stu-id="f65c7-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="f65c7-105">有关多层感知器神经网络的详细探讨不属于本文档的讨论范围。</span><span class="sxs-lookup"><span data-stu-id="f65c7-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="f65c7-106">本主题介绍该算法的基本实现，包括用于规范化输入值与输出值的方法以及用于缩减属性基数的功能选择方法。</span><span class="sxs-lookup"><span data-stu-id="f65c7-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="f65c7-107">本主题介绍可用于自定义该算法行为的参数和其他设置，并提供与查询该模型有关的其他信息的链接。</span><span class="sxs-lookup"><span data-stu-id="f65c7-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="f65c7-108">Microsoft 神经网络算法的实现</span><span class="sxs-lookup"><span data-stu-id="f65c7-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="f65c7-109">在多层感知器神经网络中，每个神经元可接收一个或多个输入，并产生一个或多个相同的输出。</span><span class="sxs-lookup"><span data-stu-id="f65c7-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="f65c7-110">每个输出都是对神经元的输入之和的简单非线性函数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="f65c7-111">输入将从输入层中的节点传递到隐藏层中的节点，然后再从隐藏层传递到输出层；同一层中的神经元之间没有连接。</span><span class="sxs-lookup"><span data-stu-id="f65c7-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="f65c7-112">如果像逻辑回归模型那样没有隐藏层，则输入将会直接从输入层中的节点传递到输出层中的节点。</span><span class="sxs-lookup"><span data-stu-id="f65c7-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="f65c7-113">在使用 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 神经网络算法创建的神经网络中，存在三种类型的神经元：</span><span class="sxs-lookup"><span data-stu-id="f65c7-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="f65c7-114">输入神经元提供数据挖掘模型的输入属性值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="f65c7-115">对于离散的输入属性，输入神经元通常代表输入属性的单个状态。</span><span class="sxs-lookup"><span data-stu-id="f65c7-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="f65c7-116">如果定型数据包含输入属性的 Null 值，则缺失的值也包括在内。</span><span class="sxs-lookup"><span data-stu-id="f65c7-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="f65c7-117">具有两个以上状态的离散输入属性会为每个状态生成一个输入神经元，如果定型数据中存在 Null 值，还会为缺失的状态生成一个输入神经元。</span><span class="sxs-lookup"><span data-stu-id="f65c7-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="f65c7-118">一个连续的输入属性将生成两个输入神经元：一个用于缺失的状态，一个用于连续属性自身的值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="f65c7-119">输入神经元可向一个或多个隐藏神经元提供输入。</span><span class="sxs-lookup"><span data-stu-id="f65c7-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="f65c7-120">隐藏神经元接收来自输入神经元的输入，并向输出神经元提供输出。</span><span class="sxs-lookup"><span data-stu-id="f65c7-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="f65c7-121">输出神经元代表数据挖掘模型的可预测属性值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="f65c7-122">对于离散输入属性，输出神经元通常代表可预测属性的单个预测状态，其中包括缺失的值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="f65c7-123">例如，一个二进制可预测属性可生成一个输出节点，该节点说明缺失的或现有的状态，以指示该属性是否存在值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="f65c7-124">一个用作可预测属性的布尔列可生成三个输出神经元：一个用于 True 值，一个用于 False 值，一个用于缺失的或现有的状态。</span><span class="sxs-lookup"><span data-stu-id="f65c7-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="f65c7-125">具有两种以上状态的离散可预测属性可为每个状态生成一个输出神经元，并为缺失的或现有的状态生成一个输出神经元。</span><span class="sxs-lookup"><span data-stu-id="f65c7-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="f65c7-126">一个连续的可预测列将生成两个输出神经元：一个用于缺失的或现有的状态，一个用于连续列自身的值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="f65c7-127">如果检查可预测列集时生成的输出神经元多于 500 个，则 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 将在挖掘模型中生成一个新网络，用于代表超出部分的输出神经元。</span><span class="sxs-lookup"><span data-stu-id="f65c7-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="f65c7-128">根据其所在网络层的不同，神经元接收来自其他神经元或来自其他数据的输入。</span><span class="sxs-lookup"><span data-stu-id="f65c7-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="f65c7-129">输入神经元接收来自原始数据的输入。</span><span class="sxs-lookup"><span data-stu-id="f65c7-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="f65c7-130">隐藏神经元和输出神经元接收来自神经网络中其他神经元的输出的输入。</span><span class="sxs-lookup"><span data-stu-id="f65c7-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="f65c7-131">输入在神经元之间建立了关系，而这些关系可用作分析特定事例集时的路径。</span><span class="sxs-lookup"><span data-stu-id="f65c7-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="f65c7-132">每个输入都分配有一个称为“权重” \*\* 的值，此值用于说明该特定输入对于隐藏神经元或输出神经元的相关性或重要性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="f65c7-133">分配给一个输入的权重越大，则该输入值的相关性或重要性越高。</span><span class="sxs-lookup"><span data-stu-id="f65c7-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="f65c7-134">权重可以是负值，表示输入可能抑制而不是激活特定神经元。</span><span class="sxs-lookup"><span data-stu-id="f65c7-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="f65c7-135">每个输入的值都会与其权重相乘，以强调输入对特定神经元的重要性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="f65c7-136">对于负权重，输入值与权重相乘的结果是降低重要性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="f65c7-137">每个神经元都分配有一个称为激活函数\*\* 的简单非线性函数，用于说明特定神经元对于神经网络层的相关性或重要性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="f65c7-138">隐藏神经元使用双曲正切\*\* 函数 (tanh) 作为其激活函数，而输出神经元使用 sigmoid \*\* 函数作为其激活函数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="f65c7-139">这两个函数都是非线性连续函数，允许神经网络在输入神经元和输出神经元之间建立非线性关系模型。</span><span class="sxs-lookup"><span data-stu-id="f65c7-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="f65c7-140">定型神经网络</span><span class="sxs-lookup"><span data-stu-id="f65c7-140">Training Neural Networks</span></span>  
 <span data-ttu-id="f65c7-141">定型使用 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 神经网络算法的数据挖掘模型时，会涉及到若干个步骤。</span><span class="sxs-lookup"><span data-stu-id="f65c7-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="f65c7-142">这些步骤与为算法参数指定的值紧密相关。</span><span class="sxs-lookup"><span data-stu-id="f65c7-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="f65c7-143">算法首先会进行评估并从数据源提取定型数据。</span><span class="sxs-lookup"><span data-stu-id="f65c7-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="f65c7-144">定型数据的百分比（称为“维持数据” \*\*）会被保留，用于评估该网络的准确性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="f65c7-145">在整个定型过程中，在每次迭代定型数据后，都会立即对网络进行评估。</span><span class="sxs-lookup"><span data-stu-id="f65c7-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="f65c7-146">准确性不再提高时，定型过程便会停止。</span><span class="sxs-lookup"><span data-stu-id="f65c7-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="f65c7-147">SAMPLE_SIZE\*\* 和 HOLDOUT_PERCENTAGE\*\* 参数的值用于确定定型数据中作为样本的事例数以及保留供维持数据使用的事例数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="f65c7-148">HOLDOUT_SEED\*\* 参数的值用于随机确定保留供维持数据使用的各个事例。</span><span class="sxs-lookup"><span data-stu-id="f65c7-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="f65c7-149">这些算法参数与应用于挖掘结构以定义测试数据集的属性 HOLDOUT_SIZE 和 HOLDOUT_SEED 不同。</span><span class="sxs-lookup"><span data-stu-id="f65c7-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="f65c7-150">接下来，该算法将确定挖掘模型支持的网络的数目以及复杂性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="f65c7-151">如果挖掘模型包含一个或多个仅用于预测的属性，算法将创建一个代表所有这些属性的单一网络。</span><span class="sxs-lookup"><span data-stu-id="f65c7-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="f65c7-152">如果挖掘模型包含一个或多个同时用于输入和预测的属性，则该算法提供程序将为其中的每个属性构建一个网络。</span><span class="sxs-lookup"><span data-stu-id="f65c7-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="f65c7-153">对于具有离散值的输入属性和可预测属性，每个输入或输出神经元各自表示单个状态。</span><span class="sxs-lookup"><span data-stu-id="f65c7-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="f65c7-154">对于具有连续值的输入属性和可预测属性，每个输入或输出神经元分别表示该属性值的范围和分布。</span><span class="sxs-lookup"><span data-stu-id="f65c7-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="f65c7-155">每种情况下支持的最大状态数取决于 MAXIMUM_STATES\*\* 算法参数的值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="f65c7-156">如果某一特定属性的状态数超过 MAXIMUM_STATES\*\* 算法参数的值，则会选出该属性最常用或相关性最高的那些状态，数目是所允许的最大状态数，剩下的状态作为缺失的值分为一组，用于分析。</span><span class="sxs-lookup"><span data-stu-id="f65c7-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="f65c7-157">然后，该算法使用 HIDDEN_NODE_RATIO\*\* 参数的值来确定要为隐藏层创建的神经元的初始数目。</span><span class="sxs-lookup"><span data-stu-id="f65c7-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="f65c7-158">可以将 HIDDEN_NODE_RATIO\*\* 设置为 0，以避免在该算法为挖掘模型生成的网络中创建隐藏层，以便将神经网络作为逻辑回归处理。</span><span class="sxs-lookup"><span data-stu-id="f65c7-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="f65c7-159">算法提供程序通过接受之前保留的定型数据集并将维持数据中的每个事例的实际已知值与网络的预测进行比较，即通过一个称为“批学习 \*\*”的进程来同时迭代计算整个网络的所有输入的权重。</span><span class="sxs-lookup"><span data-stu-id="f65c7-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="f65c7-160">该算法处理了整个定型数据集后，将检查每个神经元的预测值和实际值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="f65c7-161">该算法将计算错误程度（如果有错误），并调整与神经元输入关联的权重，并通过一个称为“回传” \*\* 的过程从输出神经元返回到输入神经元。</span><span class="sxs-lookup"><span data-stu-id="f65c7-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="f65c7-162">然后，该算法对整个定型数据集重复该过程。</span><span class="sxs-lookup"><span data-stu-id="f65c7-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="f65c7-163">该算法支持多个权重和输出神经元，因此这个共轭梯度算法用于引导定型过程来分配和计算输入权重。</span><span class="sxs-lookup"><span data-stu-id="f65c7-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="f65c7-164">有关共轭梯度算法的探讨不属于本文档的讨论范围。</span><span class="sxs-lookup"><span data-stu-id="f65c7-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="f65c7-165">特征选择</span><span class="sxs-lookup"><span data-stu-id="f65c7-165">Feature Selection</span></span>  
 <span data-ttu-id="f65c7-166">如果输入属性数大于参数 MAXIMUM_INPUT_ATTRIBUTES\*\* 的值，或可预测属性数大于参数 MAXIMUM_OUTPUT_ATTRIBUTES\*\* 的值，则会使用功能选择算法来降低挖掘模型中包含的网络的复杂性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="f65c7-167">功能选择可以将输入属性或可预测属性的数目减少到与该模型在统计上最相关的数目。</span><span class="sxs-lookup"><span data-stu-id="f65c7-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="f65c7-168">所有 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 数据挖掘算法都会自动使用功能选择来改善分析效果以及减轻处理工作量。</span><span class="sxs-lookup"><span data-stu-id="f65c7-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="f65c7-169">神经网络模型中用于功能选择的方法取决于属性的数据类型。</span><span class="sxs-lookup"><span data-stu-id="f65c7-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="f65c7-170">下表列出了用于神经网络模型的功能选择方法，以及用于逻辑回归算法（基于神经网络算法）的功能选择方法，以供参考。</span><span class="sxs-lookup"><span data-stu-id="f65c7-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="f65c7-171">算法</span><span class="sxs-lookup"><span data-stu-id="f65c7-171">Algorithm</span></span>|<span data-ttu-id="f65c7-172">分析方法</span><span class="sxs-lookup"><span data-stu-id="f65c7-172">Method of analysis</span></span>|<span data-ttu-id="f65c7-173">注释</span><span class="sxs-lookup"><span data-stu-id="f65c7-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="f65c7-174">神经网络</span><span class="sxs-lookup"><span data-stu-id="f65c7-174">Neural Network</span></span>|<span data-ttu-id="f65c7-175">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="f65c7-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="f65c7-176">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="f65c7-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="f65c7-177">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="f65c7-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="f65c7-178">使用统一先验的 Bayesian Dirichlet（默认）</span><span class="sxs-lookup"><span data-stu-id="f65c7-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="f65c7-179">只要数据包含连续列，神经网络算法就可同时使用基于平均信息量的方法和 Bayesian 计分方法。</span><span class="sxs-lookup"><span data-stu-id="f65c7-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="f65c7-180">默认。</span><span class="sxs-lookup"><span data-stu-id="f65c7-180">Default.</span></span>|  
|<span data-ttu-id="f65c7-181">逻辑回归</span><span class="sxs-lookup"><span data-stu-id="f65c7-181">Logistic Regression</span></span>|<span data-ttu-id="f65c7-182">兴趣性分数</span><span class="sxs-lookup"><span data-stu-id="f65c7-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="f65c7-183">Shannon 平均信息量</span><span class="sxs-lookup"><span data-stu-id="f65c7-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="f65c7-184">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="f65c7-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="f65c7-185">使用统一先验的 Bayesian Dirichlet（默认）</span><span class="sxs-lookup"><span data-stu-id="f65c7-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="f65c7-186">因为无法将参数传递给此算法来控制功能选择行为，所以将使用默认值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="f65c7-187">因此，如果所有属性均为离散或离散化属性，则默认值为 BDEU。</span><span class="sxs-lookup"><span data-stu-id="f65c7-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="f65c7-188">控制神经网络模型的功能选择的算法参数为 MAXIMUM_INPUT_ATTRIBUTES、MAXIMUM_OUTPUT_ATTRIBUTES 和 MAXIMUM_STATES。</span><span class="sxs-lookup"><span data-stu-id="f65c7-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="f65c7-189">您也可以通过设置 HIDDEN_NODE_RATIO 参数来控制隐藏层的数目。</span><span class="sxs-lookup"><span data-stu-id="f65c7-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="f65c7-190">计分方法</span><span class="sxs-lookup"><span data-stu-id="f65c7-190">Scoring Methods</span></span>  
 <span data-ttu-id="f65c7-191">“计分”\*\* 是规范化的一种，在为神经网络模型定型的上下文中，计分表示将值（如离散文本标签）转换为可与其他输入类型进行比较且可在网络中计算权重的值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="f65c7-192">例如，如果一个输入属性为 Gender，其可能的值为 Male 和 Female，另一个输入属性为 Income，其值范围可变，这两个属性的值不可直接比较，因此，必须编码到共同的范围，才能计算权重。</span><span class="sxs-lookup"><span data-stu-id="f65c7-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="f65c7-193">计分就是将这类输入规范化为数字值的过程：尤其是规范化为概率范围。</span><span class="sxs-lookup"><span data-stu-id="f65c7-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="f65c7-194">用于规范化的函数还有助于使输入值在统一尺度分布得更加均匀，从而避免极值扭曲分析结果。</span><span class="sxs-lookup"><span data-stu-id="f65c7-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="f65c7-195">神经网络的输出也会进行编码。</span><span class="sxs-lookup"><span data-stu-id="f65c7-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="f65c7-196">如果输出是单一目标（即预测），或者是仅用于预测而不用于输入的多个目标，模型将创建单一网络，似乎没有必要规范化输出值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="f65c7-197">但是，如果有多个属性用于输入和预测，则模型必须创建多个网络；因此，所有值都必须规范化，输出也必须在退出网络时进行编码。</span><span class="sxs-lookup"><span data-stu-id="f65c7-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="f65c7-198">输入的编码基于对定型事例中的所有离散值进行求和以及对这些值乘以其权值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="f65c7-199">这称为“加权和” \*\*，它会传递给隐藏层的激活函数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="f65c7-200">编码使用 z-score，如下所示：</span><span class="sxs-lookup"><span data-stu-id="f65c7-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="f65c7-201">**离散值**</span><span class="sxs-lookup"><span data-stu-id="f65c7-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="f65c7-202">μ = p-状态的前一概率</span><span class="sxs-lookup"><span data-stu-id="f65c7-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="f65c7-203">StdDev  = sqrt(p(1-p))</span><span class="sxs-lookup"><span data-stu-id="f65c7-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="f65c7-204">**连续值**</span><span class="sxs-lookup"><span data-stu-id="f65c7-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="f65c7-205">值存在 = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="f65c7-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="f65c7-206">无现有值 =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="f65c7-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="f65c7-207">对值进行编码后，将会对输入进行加权求和，权值为网络边缘。</span><span class="sxs-lookup"><span data-stu-id="f65c7-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="f65c7-208">对输出的编码使用 Sigmoid 函数，其所具有的特性使其对预测非常有益。</span><span class="sxs-lookup"><span data-stu-id="f65c7-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="f65c7-209">其中一个特性是，无论原始值如何计量，也无论值为正为负，此函数的输出始终在 0 和 1 之间，正好适合于计算概率。</span><span class="sxs-lookup"><span data-stu-id="f65c7-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="f65c7-210">另一个非常有用的特性是，Sigmoid 函数有平滑的效果，值离转折点越远，值的概率会越趋近 0 或 1，但缓慢得多。</span><span class="sxs-lookup"><span data-stu-id="f65c7-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="f65c7-211">自定义神经网络算法</span><span class="sxs-lookup"><span data-stu-id="f65c7-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="f65c7-212">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 神经网络算法支持多个参数，这些参数可以影响生成的挖掘模型的行为、性能和准确性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="f65c7-213">您还可以通过对列设置建模标志来修改模型处理数据的方式，或者通过设置分布标志来指定列中值的处理方式。</span><span class="sxs-lookup"><span data-stu-id="f65c7-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="f65c7-214">设置算法参数</span><span class="sxs-lookup"><span data-stu-id="f65c7-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="f65c7-215">下表介绍可用于 Microsoft 神经网络算法的参数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="f65c7-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="f65c7-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="f65c7-217">指定隐藏神经元相对于输入和输出神经元的比率。</span><span class="sxs-lookup"><span data-stu-id="f65c7-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="f65c7-218">以下公式可确定隐藏层中神经元的初始数目：</span><span class="sxs-lookup"><span data-stu-id="f65c7-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="f65c7-219">HIDDEN_NODE_RATIO \* SQRT（总输入神经元 \* 总输出神经元）</span><span class="sxs-lookup"><span data-stu-id="f65c7-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="f65c7-220">默认值为 4.0。</span><span class="sxs-lookup"><span data-stu-id="f65c7-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="f65c7-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="f65c7-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="f65c7-222">指定定型数据中用于计算维持错误的事例的百分比，定型挖掘模型时的停止条件中将用到此百分比。</span><span class="sxs-lookup"><span data-stu-id="f65c7-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="f65c7-223">默认值为 30。</span><span class="sxs-lookup"><span data-stu-id="f65c7-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="f65c7-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="f65c7-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="f65c7-225">指定一个数字，用作在算法随机确定维持数据时伪随机生成器的种子。</span><span class="sxs-lookup"><span data-stu-id="f65c7-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="f65c7-226">如果此参数设置为 0，算法将基于挖掘模型的名称生成种子，以保证重新处理期间模型内容的一致性。</span><span class="sxs-lookup"><span data-stu-id="f65c7-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="f65c7-227">默认值为 0。</span><span class="sxs-lookup"><span data-stu-id="f65c7-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="f65c7-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="f65c7-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="f65c7-229">确定在应用功能选择前，可应用于算法的输入属性的最大数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="f65c7-230">如果将此值设置为 0，则为输入属性禁用功能选择。</span><span class="sxs-lookup"><span data-stu-id="f65c7-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="f65c7-231">默认值为 255。</span><span class="sxs-lookup"><span data-stu-id="f65c7-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="f65c7-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="f65c7-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="f65c7-233">确定在应用功能选择前，可应用于算法的输出属性的最大数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="f65c7-234">如果将此值设置为 0，则为输出属性禁用功能选择。</span><span class="sxs-lookup"><span data-stu-id="f65c7-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="f65c7-235">默认值为 255。</span><span class="sxs-lookup"><span data-stu-id="f65c7-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="f65c7-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="f65c7-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="f65c7-237">指定算法支持的每个属性的离散状态的最大数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="f65c7-238">如果特定属性的状态数大于为该参数指定的数，则算法将使用该属性最普遍的状态并将剩余状态作为缺失的状态处理。</span><span class="sxs-lookup"><span data-stu-id="f65c7-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="f65c7-239">默认值为 100。</span><span class="sxs-lookup"><span data-stu-id="f65c7-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="f65c7-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="f65c7-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="f65c7-241">指定用来给模型定型的事例数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="f65c7-242">该算法使用此数或 HOLDOUT_PERCENTAGE 参数指定的包含在维持数据中的事例总数的百分比，取两者中较小的一个。</span><span class="sxs-lookup"><span data-stu-id="f65c7-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="f65c7-243">换言之，如果 HOLDOUT_PERCENTAGE 设置为 30，则该算法将使用此参数的值或事例总数的 70% 的值，取两者中较小的一个。</span><span class="sxs-lookup"><span data-stu-id="f65c7-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="f65c7-244">默认值为 10000。</span><span class="sxs-lookup"><span data-stu-id="f65c7-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="f65c7-245">建模标志</span><span class="sxs-lookup"><span data-stu-id="f65c7-245">Modeling Flags</span></span>  
 <span data-ttu-id="f65c7-246">支持将以下建模标志与 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 神经网络算法配合使用。</span><span class="sxs-lookup"><span data-stu-id="f65c7-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="f65c7-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="f65c7-247">NOT NULL</span></span>  
 <span data-ttu-id="f65c7-248">指示该列不能包含 Null。</span><span class="sxs-lookup"><span data-stu-id="f65c7-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="f65c7-249">如果 Analysis Services 在模型定型过程中遇到 Null 值，将会导致错误。</span><span class="sxs-lookup"><span data-stu-id="f65c7-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="f65c7-250">适用于挖掘结构列。</span><span class="sxs-lookup"><span data-stu-id="f65c7-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="f65c7-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="f65c7-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="f65c7-252">指示该模型应仅考虑是否存在属性值或是否缺失值。</span><span class="sxs-lookup"><span data-stu-id="f65c7-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="f65c7-253">不考虑值具体为多少。</span><span class="sxs-lookup"><span data-stu-id="f65c7-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="f65c7-254">适用于挖掘模型列。</span><span class="sxs-lookup"><span data-stu-id="f65c7-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="f65c7-255">分布标志</span><span class="sxs-lookup"><span data-stu-id="f65c7-255">Distribution Flags</span></span>  
 <span data-ttu-id="f65c7-256">支持以下分布标志与 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 神经网络算法配合使用。</span><span class="sxs-lookup"><span data-stu-id="f65c7-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="f65c7-257">这些标志仅用作对模型的提示；如果算法检测到不同的分布，算法将使用所发现的分布，而不使用提示中提供的分布。</span><span class="sxs-lookup"><span data-stu-id="f65c7-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="f65c7-258">普通</span><span class="sxs-lookup"><span data-stu-id="f65c7-258">Normal</span></span>  
 <span data-ttu-id="f65c7-259">指示应将列中的值视为表示正态分布（或称高斯分布）。</span><span class="sxs-lookup"><span data-stu-id="f65c7-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="f65c7-260">Uniform</span><span class="sxs-lookup"><span data-stu-id="f65c7-260">Uniform</span></span>  
 <span data-ttu-id="f65c7-261">指示应将列中的值视为均匀分布；即，任何值的概率都基本相等，且为值的总数的函数。</span><span class="sxs-lookup"><span data-stu-id="f65c7-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="f65c7-262">Log Normal</span><span class="sxs-lookup"><span data-stu-id="f65c7-262">Log Normal</span></span>  
 <span data-ttu-id="f65c7-263">指示应将列中的值视为按对数正态\*\* 曲线分布，即值的对数为正态分布。</span><span class="sxs-lookup"><span data-stu-id="f65c7-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="f65c7-264">要求</span><span class="sxs-lookup"><span data-stu-id="f65c7-264">Requirements</span></span>  
 <span data-ttu-id="f65c7-265">神经网络模型必须包含至少一个输入列和一个输出列。</span><span class="sxs-lookup"><span data-stu-id="f65c7-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="f65c7-266">输入列和可预测列</span><span class="sxs-lookup"><span data-stu-id="f65c7-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="f65c7-267">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 神经网络算法支持下表中列出的特定输入列和可预测列。</span><span class="sxs-lookup"><span data-stu-id="f65c7-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="f65c7-268">列</span><span class="sxs-lookup"><span data-stu-id="f65c7-268">Column</span></span>|<span data-ttu-id="f65c7-269">内容类型</span><span class="sxs-lookup"><span data-stu-id="f65c7-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="f65c7-270">输入属性</span><span class="sxs-lookup"><span data-stu-id="f65c7-270">Input attribute</span></span>|<span data-ttu-id="f65c7-271">Continuous、Cyclical、Discrete、Discretized、Key、Table 和 Ordered</span><span class="sxs-lookup"><span data-stu-id="f65c7-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="f65c7-272">可预测属性</span><span class="sxs-lookup"><span data-stu-id="f65c7-272">Predictable attribute</span></span>|<span data-ttu-id="f65c7-273">Continuous、Cyclical、Discrete、Discretized 和 Ordered</span><span class="sxs-lookup"><span data-stu-id="f65c7-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="f65c7-274">支持 Cyclical 和 Ordered 内容类型，但算法会将它们视为离散值，不会进行特殊处理。</span><span class="sxs-lookup"><span data-stu-id="f65c7-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="f65c7-275">另请参阅</span><span class="sxs-lookup"><span data-stu-id="f65c7-275">See Also</span></span>  
 <span data-ttu-id="f65c7-276">[Microsoft 神经网络算法](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="f65c7-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="f65c7-277">[&#40;Analysis Services 数据挖掘的神经网络模型的挖掘模型内容&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="f65c7-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="f65c7-278">神经网络模型查询示例</span><span class="sxs-lookup"><span data-stu-id="f65c7-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
